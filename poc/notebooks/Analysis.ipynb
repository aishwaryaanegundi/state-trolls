{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# file sizes, number of hits and cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "filenames = glob.glob('/INET/state-trolls/work/state-trolls/reddit_dataset/comments/scores/RC_2016-10.bz2.decompressed/*txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in filenames:\n",
    "    try:\n",
    "        with open(name , 'r') as content_file:\n",
    "            content = content_file.read()\n",
    "            json_data = content.replace('][',',')\n",
    "            j_object = json.loads(json_data)\n",
    "            j_df = pd.DataFrame(j_object)\n",
    "            counts = collections.Counter(j_df['tweet_id'].tolist())\n",
    "            counts_sorted = counts.most_common()\n",
    "            c_df = pd.DataFrame(counts_sorted, columns=['tweet_id', 'frequency'])\n",
    "            final_df = final_df.append(c_df, ignore_index = True) \n",
    "            final_df['t_f'] = final_df.groupby(['tweet_id'])['frequency'].transform('sum')\n",
    "            final_df = final_df.drop_duplicates(subset=['tweet_id'])\n",
    "            del final_df['frequency']\n",
    "            final_df.columns = ['tweet_id', 'frequency']\n",
    "#             print(final_df.head())\n",
    "#             print(len(final_df))\n",
    "            i = i + 1\n",
    "            print(i)\n",
    "    except:\n",
    "        no_hits_files = no_hits_files + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_hits_files)\n",
    "print(hits_files)\n",
    "print(no_of_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "with open('/INET/state-trolls/work/state-trolls/reddit_dataset/comments/scores/RC_2016-10.bz2.decompressed/7076_scores_stsb.txt' , 'r') as content_file:\n",
    "        size = content_file.tell()\n",
    "        content = content_file.read()\n",
    "        json_data = content.replace('][',',')\n",
    "        j_object = json.loads(json_data)\n",
    "        j_df = pd.DataFrame(j_object)\n",
    "        print(j_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDF plot and theme distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "counts, bin_edges = np.histogram (sims, bins=20, normed=False)\n",
    "cdf = np.cumsum (counts)\n",
    "plt.plot(bin_edges[1:], cdf/cdf[-1])\n",
    "plt.xlabel('cosine similarity')\n",
    "plt.ylabel('3M hits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./clustering/annotated/combined_annotated_updated1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_data = data[data['relevance']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "counts = []\n",
    "labels = ['Muslims', 'Racism', 'Elections','Michel Obama', 'Congress','Islam', 'Zimbabwe', 'Cop Violence',\n",
    "         'White House', 'UK', 'Republicans', ' Barack Obama', 'Hilary Clinton', 'Trump', 'Protests/Rallies',\n",
    "         'Biden', 'Democratics', 'US Gov', 'Gen Politics', 'LGBTQ', 'ISIS', 'China', 'Kenya', 'Russia', 'Korea',\n",
    "         'Syria', 'Abortion', 'Miscellaneous','Sanders/Cruz','Israel','Liberals', 'Iran', 'Germany', 'Greek', \n",
    "          'Flint water crisis','Turkey','Pakistan','Bill Cosby','Gun law', 'James Comey','Climate change',\n",
    "          'India','Columbia','Ferguson','No cyber censorship movement',\n",
    "          'Brazil','Saudi -Yemen','Belgium','Panama','Religion','Europe','Mexico','Baltimore',\n",
    "          'Paris climate deal','Afghan  - Taliban','Australia','Cuba','Italy']\n",
    "for i in range(1,59):\n",
    "    shape = relevant_data[relevant_data['theme'] == i].shape\n",
    "    counts.append(shape[0])\n",
    "    count =  count + shape[0]\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "plt.style.use('ggplot')\n",
    "df = pd.DataFrame()\n",
    "df['counts'] = counts\n",
    "df['labels'] = labels\n",
    "df = df.sort_values('counts', ascending=False)\n",
    "df.set_index('labels', inplace=True)\n",
    "fig, ax = plt.subplots(figsize=(7,10))\n",
    "df.plot(kind='barh', legend = False, ax=ax)\n",
    "ax.set_xlabel('Number of Tweets')\n",
    "ax.set_ylabel('Themes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "import emoji\n",
    "import string\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# list of all the dataset files\n",
    "dataset_paths = [\"../../datasets/russia_052020_tweets_csv_hashed_2.csv\", \n",
    "         \"../../datasets/russian_linked_tweets_csv_hashed.csv\", \n",
    "         \"../../datasets/ira_tweets_csv_hashed.csv\", \n",
    "         \"../../datasets/russia_201906_1_tweets_csv_hashed.csv\"]\n",
    "\n",
    "# path to store the entire combined dataset\n",
    "combined_dataset_path = \"../datasets/russian_trolls.csv\"\n",
    "\n",
    "# returns a pandas dataframe consisting of entries from all the dataset files\n",
    "def get_combined_dataset(paths):\n",
    "    data = pd.concat((pd.read_csv(file) for file in tqdm(paths)))\n",
    "    return data\n",
    "\n",
    "data = get_combined_dataset(dataset_paths)\n",
    "print(\"Number of tweets in the dataset: \", data.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get samples for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_annotations = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = pd.read_csv('./annotations/2016.11.annotation_data_point_95.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = adf[~adf[['tweet', 'matching_sentence']].apply(frozenset, axis=1).duplicated()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res[~res[['tweet']].apply(frozenset, axis=1).duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res[~res[['matching_sentence']].apply(frozenset, axis=1).duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_annotations = final_annotations.append(res.sample(n = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_annotations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del final_annotations['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = final_annotations.sample(n=170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf['annotation_based_on_sentence'] = 2\n",
    "fdf['annotation_based_on_post'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.read_csv('./annotation_samples_saved.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdf = pdf.append(fdf, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf.to_csv('./annotations/annotation_samples.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odf = fdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./clustering/annotated/annotation_samples_aishwarya.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odf['annotation_based_on_sentence'] = df['annotation_based_on_sentence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odf['annotation_based_on_post'] = df['annotation_based_on_post'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(odf[(odf['sim'] > 0.65) & (odf['sim'] <= 0.7)].shape)\n",
    "print(odf[(odf['sim'] > 0.7) & (odf['sim'] <= 0.75)].shape)\n",
    "print(odf[(odf['sim'] > 0.75) & (odf['sim'] <= 0.8)].shape)\n",
    "print(odf[(odf['sim'] > 0.8) & (odf['sim'] <= 0.85)].shape)\n",
    "print(odf[(odf['sim'] > 0.85) & (odf['sim'] <= 0.9)].shape)\n",
    "print(odf[(odf['sim'] > 0.9) & (odf['sim'] <= 0.95)].shape)\n",
    "print(odf[(odf['sim'] > 0.95) & (odf['sim'] <= 1.0)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odf.to_csv('./annotation_samples_saved.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odf = pd.read_csv('./annotation_samples_aishwarya.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = odf[odf['annotation_based_on_post'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matrix_0 = []\n",
    "matrix_1 = []\n",
    "from sklearn.metrics import classification_report\n",
    "target_names = ['irrelevant', 'relevant']\n",
    "for threshold in np.arange(0.65, 1.0, 0.05):\n",
    "    sdf['true-values'] = np.where(sdf['sim'] > threshold, 1, 0)\n",
    "    c = classification_report(sdf['true-values'],\n",
    "                              sdf['annotation_based_on_post'], target_names=target_names, output_dict = True)\n",
    "    matrix_0.append({'threshold': threshold, 'class': '0',\n",
    "                   'precision' : (c['irrelevant'])['precision'],\n",
    "                  'recall': (c['irrelevant'])['recall'],\n",
    "                  'f1-score': (c['irrelevant'])['f1-score'],\n",
    "                  'support': (c['irrelevant'])['support'],\n",
    "                  'accuracy': c['accuracy']})\n",
    "    matrix_1.append({'threshold': threshold, 'class': '1',\n",
    "                   'precision' : (c['relevant'])['precision'],\n",
    "                  'recall': (c['relevant'])['recall'],\n",
    "                  'f1-score': (c['relevant'])['f1-score'],\n",
    "                  'support': (c['relevant'])['support'],\n",
    "                  'accuracy': c['accuracy']})\n",
    "#     print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = pd.DataFrame(matrix_1)\n",
    "del k['support']\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "counts, bin_edges = np.histogram (sims, bins=500, normed=False)\n",
    "cdf = np.cumsum (counts)\n",
    "plt.plot(bin_edges[1:], cdf/cdf[-1])\n",
    "plt.xlabel('ratio of post length to max tweet length')\n",
    "plt.ylabel('randomized hits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cdf(list_counts, xlabel, path, leg=False, islogx=True, xlimit=False):\n",
    "    t_col = \"#235dba\"\n",
    "    g_col = \"#005916\"\n",
    "    c_col = \"#a50808\"\n",
    "    r_col = \"#ff9900\"\n",
    "    black = \"#000000\"\n",
    "    pink = \"#f442f1\"\n",
    "    t_ls = '-'\n",
    "    r_ls = '--'\n",
    "    c_ls = ':'\n",
    "    g_ls = '-.'\n",
    "\n",
    "    markers = [\".\", \"o\", \"v\", \"^\", \"<\", \">\", \"1\", \"2\"]\n",
    "    colors = [t_col, c_col, g_col, r_col, black, 'c', 'm', pink]\n",
    "    line_styles = [t_ls, r_ls, c_ls, g_ls,t_ls, r_ls, c_ls, g_ls, t_ls]\n",
    "    colors = colors[1:]\n",
    "    line_styles= line_styles[1:]\n",
    "    while(len(list_counts) > len(colors)):\n",
    "        colors = colors + shuffle(colors)\n",
    "        line_styles = line_styles + shuffle(line_styles)\n",
    "        \n",
    "    if xlimit:\n",
    "        l2 = []\n",
    "        for l in list_counts:\n",
    "            l2_1 = [x for x in l if x<=xlimit]\n",
    "            l2.append(l2_1)\n",
    "        list_counts = l2\n",
    "    \n",
    "    for l in list_counts:\n",
    "        l.sort()\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    yvals = []\n",
    "    for l in list_counts:\n",
    "        yvals.append(np.arange(len(l))/float(len(l)-1))\n",
    "    for i in range(len(list_counts)):\n",
    "        ax.plot(list_counts[i], yvals[i], color=colors[i], linestyle=line_styles[i])\n",
    "    if islogx:\n",
    "        ax.set_xscale(\"log\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('CDF')\n",
    "    plt.grid()\n",
    "    for item in ([ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(13)\n",
    "    \n",
    "    if leg:\n",
    "        plt.legend(leg, loc='best', fontsize=13)\n",
    "    \n",
    "    plt.show()\n",
    "    fig.savefig(path, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf([sims], 'ratio', './cdf_of_ratio_of_tweet_lengths.pdf', islogx=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting samples with sim  > 0.8 along with the time difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_annotation = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_annotation = combined_annotation.append(pd.read_csv('./annotations/2016.10.annotation_data_point_8.csv'), ignore_index = True)\n",
    "combined_annotation = combined_annotation.append(pd.read_csv('./annotations/2016.10.annotation_data_point_85.csv'), ignore_index = True)\n",
    "combined_annotation = combined_annotation.append(pd.read_csv('./annotations/2016.10.annotation_data_point_9.csv'), ignore_index = True)\n",
    "combined_annotation = combined_annotation.append(pd.read_csv('./annotations/2016.10.annotation_data_point_95.csv'), ignore_index = True)\n",
    "combined_annotation = combined_annotation.append(pd.read_csv('./annotations/2016.11.annotation_data_point_8.csv'), ignore_index = True)\n",
    "combined_annotation = combined_annotation.append(pd.read_csv('./annotations/2016.11.annotation_data_point_85.csv'), ignore_index = True)\n",
    "combined_annotation = combined_annotation.append(pd.read_csv('./annotations/2016.11.annotation_data_point_9.csv'), ignore_index = True)\n",
    "combined_annotation = combined_annotation.append(pd.read_csv('./annotations/2016.11.annotation_data_point_95.csv'), ignore_index = True)\n",
    "combined_annotation = combined_annotation.append(pd.read_csv('./annotations/2017.04.annotation_data_point_8.csv'), ignore_index = True)\n",
    "combined_annotation = combined_annotation.append(pd.read_csv('./annotations/2017.04.annotation_data_point_85.csv'), ignore_index = True)\n",
    "combined_annotation = combined_annotation.append(pd.read_csv('./annotations/2017.04.annotation_data_point_9.csv'), ignore_index = True)\n",
    "combined_annotation = combined_annotation.append(pd.read_csv('./annotations/2017.04.annotation_data_point_95.csv'), ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "res = combined_annotation[~combined_annotation[['tweet_text', 'matching_sentence']].apply(frozenset, axis=1).duplicated()]\n",
    "res = res[~res[['tweet_text']].apply(frozenset, axis=1).duplicated()]\n",
    "res = res[~res[['matching_sentence']].apply(frozenset, axis=1).duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = combined_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata = data[data['tweetid'].isin(res['tweet_id'].to_list())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for idx, row in res.iterrows():\n",
    "    element = fdata[fdata['tweetid'] == row['tweet_id']]\n",
    "    time = element['tweet_time'].to_string()\n",
    "    time = time.split()\n",
    "#     print(time[1]+' '+time[2])\n",
    "    times.append(time[1]+' '+time[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del res['tweet_time']\n",
    "res['tweet_time'] = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_posts = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = pd.read_csv('/INET/state-trolls/work/state-trolls/reddit_dataset/comments/posts/posts-2016-10.csv')\n",
    "fposts = posts[posts['id'].isin(res['post_id'].to_list())]\n",
    "rel_posts = rel_posts.append(fposts, ignore_index=True)\n",
    "posts = pd.read_csv('/INET/state-trolls/work/state-trolls/reddit_dataset/comments/posts/posts-2016-11.csv')\n",
    "fposts = posts[posts['id'].isin(res['post_id'].to_list())]\n",
    "rel_posts = rel_posts.append(fposts, ignore_index=True)\n",
    "posts = pd.read_csv('/INET/state-trolls/work/state-trolls/reddit_dataset/comments/posts/posts-2017-04.csv')\n",
    "fposts = posts[posts['id'].isin(res['post_id'].to_list())]\n",
    "rel_posts = rel_posts.append(fposts, ignore_index=True)\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "times = []\n",
    "for idx, row in res.iterrows():\n",
    "    element = rel_posts[rel_posts['id'] == row['post_id']]\n",
    "    print((element['subreddit'].values))\n",
    "    time = element['created_utc'].astype('int64')\n",
    "    time = str(time).split()\n",
    "#     print(time)\n",
    "    try:\n",
    "        times.append(datetime.datetime.fromtimestamp(int(time[1])))\n",
    "    except ValueError:\n",
    "        print(time[0])\n",
    "        times.append(time[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del res['post_time']\n",
    "res['post_time'] = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res[res['post_time'] !=  'Series([],']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.to_datetime(res.tweet_time, infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del res['tweet_time']\n",
    "res['tweet_time'] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.to_datetime(res.post_time, infer_datetime_format=True)\n",
    "del res['post_time']\n",
    "res['post_time'] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = (res.post_time - res.tweet_time).astype('timedelta64[h]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['time_difference'] = td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('./annotations/combinedpoint8.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDF of time difference between post and tweets of all the hits so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_times =pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv('./relevant_tweets.csv')\n",
    "twitter_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = pd.read_csv('/INET/state-trolls/work/state-trolls/reddit_dataset/comments/posts/posts-2017-04.csv')\n",
    "reddit_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = pd.read_csv('./results/combined_hits_2017.04.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = hits.rename(columns={'tweet_time': 'post_time'})\n",
    "del hits['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = hits[hits['post_time'] != 'Series([],']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata = data[data['tweetid'].isin(hits['tweet_id'].unique())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "authors = []\n",
    "for idx, row in hits.iterrows():\n",
    "    element = fdata[fdata['tweetid'] == row['tweet_id']]\n",
    "    time = element['tweet_time'].to_string()\n",
    "    time = time.split()\n",
    "#     print(time[1]+' '+time[2])\n",
    "    times.append(time[1]+' '+time[2])\n",
    "#     authors.append(row[''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits['tweet_time'] = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits[hits['tweet_time'] == 'Series([],'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.to_datetime(hits.tweet_time, infer_datetime_format=True)\n",
    "del hits['tweet_time']\n",
    "hits['tweet_time'] = t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.to_datetime(hits.post_time, infer_datetime_format=True)\n",
    "del hits['post_time']\n",
    "hits['post_time'] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = (hits.post_time - hits.tweet_time).astype('timedelta64[h]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits['td'] = td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_times = combined_times.append(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = (combined_times.post_time - combined_times.tweet_time).astype('timedelta64[h]')\n",
    "del combined_times['td']\n",
    "combined_times['td'] = td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_times[combined_times['td'] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_times.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf([combined_times['td'].to_list()], 'time difference in seconds', './cdf_of_time_difference_hits_seconds.pdf', islogx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time difference analysis for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_td_df = pd.DataFrame()\n",
    "authors_tds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = hits['author'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for a in authors:\n",
    "    tds = combined_times[hits['author']==a]['td'].to_list()\n",
    "    avg_td = np.mean(tds)\n",
    "    authors_tds.append({'author': a, 'atd': avg_td})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(authors_tds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_td_df = pd.DataFrame(authors_tds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf([author_td_df['atd'].to_list()], 'time difference in hours', './cdf_of_avg_time_difference_per_author_log.pdf', islogx=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot cdf of posts per author with time difference 1w, 2w , 1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_hits = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = pd.read_csv('./results/combined_hits_2017.04.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_hits = combined_hits.append(hits, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = combined_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = hits.rename(columns={'tweet_time': 'post_time'})\n",
    "del hits['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = []\n",
    "for a in hits['author']:\n",
    "    s = a.split()\n",
    "    authors.append(s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del hits['author']\n",
    "hits['author'] = authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "for idx, row in hits.iterrows():\n",
    "    element = fdata[fdata['tweetid'] == row['tweet_id']]\n",
    "    time = element['tweet_time'].to_string()\n",
    "    time = time.split()\n",
    "    try:\n",
    "        times.append(time[1]+' '+time[2])\n",
    "    except IndexError:\n",
    "        times.append(time[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits['tweet_time'] = times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = hits[hits['tweet_time'] != 'Series([],']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = hits[hits['post_time'] != 'Series([],']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.to_datetime(hits.tweet_time, infer_datetime_format=True)\n",
    "del hits['tweet_time']\n",
    "hits['tweet_time'] = t\n",
    "t = pd.to_datetime(hits.post_time, infer_datetime_format=True)\n",
    "del hits['post_time']\n",
    "hits['post_time'] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = (hits.post_time - hits.tweet_time).astype('timedelta64[h]')\n",
    "hits['td'] = td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_week_hits = hits[(hits['td'] >= -168) & (hits['td'] <= 168)]\n",
    "one_week_hits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = one_week_hits.groupby('author').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onewresults = result['post_id'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_week_hits = hits[(hits['td'] >= -336) & (hits['td'] <= 336)]\n",
    "two_week_hits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = two_week_hits.groupby('author').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twowresults = result['post_id'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_month_hits = hits[(hits['td'] >= -744) & (hits['td'] <= 744)]\n",
    "one_month_hits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = one_month_hits.groupby('author').nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onemresults = result['post_id'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(onewresults))\n",
    "print(len(twowresults))\n",
    "print(len(onemresults))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf([onewresults, twowresults, onemresults], 'hits/author',leg=['1 week time difference', '2 week time difference', '1 month time difference'], path = './cdf_of_hits_per_author_for_varying_td.pdf', islogx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf([onewresults, twowresults, onemresults], 'tweets/author',leg=['1 week time difference', '2 week time difference', '1 month time difference'], path = './cdf_of_tweets_per_author_for_varying_td.pdf', islogx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf([onewresults, twowresults, onemresults], 'posts/author',leg=['1 week time difference', '2 week time difference', '1 month time difference'], path = './cdf_of_posts_per_author_for_varying_td.pdf', islogx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onew = one_week_hits.groupby('author').count()\n",
    "onew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "owr = onew[onew['td']!=1]['td'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(owr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = onew[onew['td']!=1].index.tolist()\n",
    "fow = one_week_hits[one_week_hits['author'].isin(df1)]\n",
    "print(fow.shape)\n",
    "owr = fow.groupby('author').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twow = two_week_hits.groupby('author').count()\n",
    "twow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twr = twow[twow['td']!=1]['td'].to_list()\n",
    "len(twr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = twow[twow['td']!=1].index.tolist()\n",
    "fow = two_week_hits[two_week_hits['author'].isin(df1)]\n",
    "print(fow.shape)\n",
    "twr = fow.groupby('author').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onem = one_month_hits.groupby('author').count()\n",
    "onem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omr = onem[onem['td']!=1]['td'].to_list()\n",
    "len(omr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = onem[onem['td']!=1].index.tolist()\n",
    "fow = one_month_hits[one_month_hits['author'].isin(df1)]\n",
    "print(fow.shape)\n",
    "omr = fow.groupby('author').nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf([owr, twr, omr], 'hits/author',leg=['1 week time difference', '2 week time difference', '1 month time difference'], path = './cdf_of_hits_per_author_for_varying_td_filtered.pdf', islogx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onew = one_week_hits.groupby('author').nunique()\n",
    "print(onew.shape)\n",
    "owr = onew['tweet_id'].to_list()\n",
    "twow = two_week_hits.groupby('author').nunique()\n",
    "print(twow.shape)\n",
    "twr = twow['tweet_id'].to_list()\n",
    "print(len(twr))\n",
    "onem = one_month_hits.groupby('author').nunique()\n",
    "print(onem.shape)\n",
    "omr = onem['tweet_id'].to_list()\n",
    "print(len(omr))\n",
    "plot_cdf([owr, twr, omr], 'tweets/author',leg=['1 week time difference', '2 week time difference', '1 month time difference'], path = './cdf_of_tweets_per_author_for_varying_td_filtered.pdf', islogx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf([owr['tweet_id'].tolist(), twr['tweet_id'].tolist(), omr['tweet_id'].tolist()], 'tweets/author',leg=['1 week time difference', '2 week time difference', '1 month time difference'], path = './cdf_of_tweets_per_author_for_varying_td_filtered.pdf', islogx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onew = one_week_hits.groupby('author').nunique()\n",
    "print(onew.shape)\n",
    "owr = onew['post_id'].to_list()\n",
    "twow = two_week_hits.groupby('author').nunique()\n",
    "print(twow.shape)\n",
    "twr = twow['post_id'].to_list()\n",
    "print(len(twr))\n",
    "onem = one_month_hits.groupby('author').nunique()\n",
    "print(onem.shape)\n",
    "omr = onem['post_id'].to_list()\n",
    "print(len(omr))\n",
    "plot_cdf([owr, twr, omr], 'posts/author',leg=['1 week time difference', '2 week time difference', '1 month time difference'], path = './cdf_of_posts_per_author_for_varying_td_filtered.pdf', islogx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf([owr['post_id'].tolist(), twr['post_id'].tolist(), omr['post_id'].tolist()], 'posts/author',leg=['1 week time difference', '2 week time difference', '1 month time difference'], path = './cdf_of_posts_per_author_for_varying_td_filtered.pdf', islogx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most frequently occuring tweets within one weel time difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_tweets = fow.groupby('tweet_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_tweets_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_tweets_df['tweet_id'] = frequent_tweets.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_tweets_df['count'] = frequent_tweets.post_id.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_tweets_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_tweets_df = frequent_tweets_df.sort_values(by=['count'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_data = data[data['tweetid'].isin(frequent_tweets_df['tweet_id'].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i in frequent_tweets_df['tweet_id'].to_list():\n",
    "    texts.append((related_data[related_data['tweetid'] == i]['tweet_text'].values)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_tweets_df['tweet_text'] = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_tweets_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_tweets_df = frequent_tweets_df.rename(columns={'count': 'frequency'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_tweets_df.to_csv('./results/frequent_tweets_one_month_td_for_authors_with_multiple_hits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = pd.DataFrame()\n",
    "k1 = one_month_hits.groupby(['author'])['post_id'].unique().reset_index(name=\"unique_post_ids\")\n",
    "k2 = one_month_hits.groupby(['author'])['post_id'].unique().reset_index(name=\"unique_post_ids\")\n",
    "k3 = one_month_hits.groupby(['author'])['post_id'].unique().reset_index(name=\"unique_post_ids\")\n",
    "k = k.append(k1, ignore_index = True)\n",
    "k = k.append(k2, ignore_index = True)\n",
    "k = k.append(k3, ignore_index = True)\n",
    "k = k.groupby('author').agg({'unique_post_ids':list}).reset_index()\n",
    "k.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "((k.unique_post_ids.tolist())[0])\n",
    "k = np.concatenate([((k.unique_post_ids.tolist())[0]), ((k.unique_post_ids.tolist())[0]), ((k.unique_post_ids.tolist())[0])], axis=0)\n",
    "np.unique(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = one_month_hits.groupby(['author'])[\"td\"].count().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = k.append( one_month_hits.groupby(['author'])[\"td\"].count().reset_index(name=\"count\"), ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_month_hits.groupby(['author'])[\"td\"].mean().reset_index(name=\"average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.groupby(['author'])['count'].sum().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import random\n",
    "import emoji\n",
    "import string\n",
    "import json\n",
    "import nltk\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine\n",
    "import collections\n",
    "import glob as glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import json\n",
    "import tldextract\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import gensim\n",
    "from gensim.parsing import PorterStemmer\n",
    "import networkx as nx\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import ast\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "t_col = \"#235dba\"\n",
    "g_col = \"#005916\"\n",
    "c_col = \"#a50808\"\n",
    "r_col = \"#ff9900\"\n",
    "black = \"#000000\"\n",
    "pink = \"#f442f1\"\n",
    "t_ls = '-'\n",
    "r_ls = '--'\n",
    "c_ls = ':'\n",
    "g_ls = '-.'\n",
    "colors = [t_col, c_col, g_col, r_col, black, 'c', 'm', pink]\n",
    "line_styles = [t_ls, r_ls, c_ls, g_ls,t_ls, r_ls, c_ls, g_ls, t_ls]\n",
    "\n",
    "# list of all the dataset files\n",
    "dataset_paths = [\"../../datasets/russia_052020_tweets_csv_hashed_2.csv\", \n",
    "         \"../../datasets/russian_linked_tweets_csv_hashed.csv\", \n",
    "         \"../../datasets/ira_tweets_csv_hashed.csv\", \n",
    "         \"../../datasets/russia_201906_1_tweets_csv_hashed.csv\"]\n",
    "\n",
    "# path to store the entire combined dataset\n",
    "combined_dataset_path = \"../datasets/russian_trolls.csv\"\n",
    "\n",
    "# returns a pandas dataframe consisting of entries from all the dataset files\n",
    "def get_combined_dataset(paths):\n",
    "    data = pd.concat((pd.read_csv(file) for file in tqdm(paths)))\n",
    "    return data\n",
    "\n",
    "data = get_combined_dataset(dataset_paths)\n",
    "print(\"Number of tweets in the dataset: \", data.shape[0])\n",
    "\n",
    "def plot_cdf(list_counts, xlabel, path, leg=False, islogx=True, xlimit=False):\n",
    "    t_col = \"#235dba\"\n",
    "    g_col = \"#005916\"\n",
    "    c_col = \"#a50808\"\n",
    "    r_col = \"#ff9900\"\n",
    "    black = \"#000000\"\n",
    "    pink = \"#f442f1\"\n",
    "    t_ls = '-'\n",
    "    r_ls = '--'\n",
    "    c_ls = ':'\n",
    "    g_ls = '-.'\n",
    "\n",
    "    markers = [\".\", \"o\", \"v\", \"^\", \"<\", \">\", \"1\", \"2\"]\n",
    "    colors = [t_col, c_col, g_col, r_col, black, 'c', 'm', pink]\n",
    "    line_styles = [t_ls, r_ls, c_ls, g_ls,t_ls, r_ls, c_ls, g_ls, t_ls]\n",
    "    colors = colors[1:]\n",
    "    line_styles= line_styles[1:]\n",
    "    while(len(list_counts) > len(colors)):\n",
    "        colors = colors + shuffle(colors)\n",
    "        line_styles = line_styles + shuffle(line_styles)\n",
    "        \n",
    "    if xlimit:\n",
    "        l2 = []\n",
    "        for l in list_counts:\n",
    "            l2_1 = [x for x in l if x<=xlimit]\n",
    "            l2.append(l2_1)\n",
    "        list_counts = l2\n",
    "    \n",
    "    for l in list_counts:\n",
    "        l.sort()\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    yvals = []\n",
    "    for l in list_counts:\n",
    "        yvals.append(np.arange(len(l))/float(len(l)-1))\n",
    "    for i in range(len(list_counts)):\n",
    "        ax.plot(list_counts[i], yvals[i], color=colors[i], linestyle=line_styles[i])\n",
    "    if islogx:\n",
    "        ax.set_xscale(\"log\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('CDF')\n",
    "    plt.grid()\n",
    "    for item in ([ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(13)\n",
    "    \n",
    "    if leg:\n",
    "        plt.legend(leg, loc='best', fontsize=13)\n",
    "    \n",
    "    plt.show()\n",
    "    fig.savefig(path, bbox_inches='tight')\n",
    "\n",
    "# Collect all hits above 0.8 cosine similarity\n",
    "months = ['2016-02','2016-03','2016-04','2016-05','2016-10','2017-01','2017-02','2017-03','2017-04', '2016-12', '2016-06']\n",
    "\n",
    "def get_relevant_hits(month):\n",
    "    combined_hits = pd.DataFrame()\n",
    "    reddit_data = pd.read_csv('/INET/state-trolls/work/state-trolls/reddit_dataset/comments/posts/posts-'+ month +'.csv')\n",
    "    print('reddit data shape',reddit_data.shape)\n",
    "    count = 0\n",
    "    for f in glob.glob('../../reddit_dataset/comments/scores/RC_'+ month +'.bz2.decompressed/*.txt'):\n",
    "        hits = []\n",
    "        with open(f , 'r') as content_file:\n",
    "            content = content_file.read()\n",
    "            json_data = content.replace('][',',')\n",
    "            j_object = json.loads(json_data)\n",
    "            json_df = pd.DataFrame(j_object)\n",
    "            j_df = json_df[(json_df['cosine_similarity'] > 0.8)]\n",
    "            fdata = data[data['tweetid'].isin(j_df['tweet_id'].to_list())]\n",
    "            fposts = reddit_data[reddit_data['id'].isin(j_df['post_id'].to_list())]\n",
    "            print('fposts  shape', fposts.shape)\n",
    "            for index, row in j_df.iterrows():\n",
    "                post = fposts[fposts['id'] == row['post_id']]\n",
    "                if not (post.empty):\n",
    "                    try:\n",
    "                        tweet = fdata[fdata['tweetid'] == row['tweet_id']]\n",
    "                        post_time = (post['created_utc'].values)[0]\n",
    "                        post_time = (datetime.datetime.fromtimestamp(post_time))\n",
    "                        element = fdata[fdata['tweetid'] == row['tweet_id']]\n",
    "                        tweet_time = (element['tweet_time'].values)[0]\n",
    "                        hits.append({'tweet_id': row['tweet_id'], 'post_id': row['post_id'], \n",
    "                                     'post_author': (post['author'].values)[0],\n",
    "                                     'post_time': post_time, 'tweet_time': tweet_time})\n",
    "                    except IndexError:\n",
    "#                         print(post)\n",
    "                        print(post['created_utc'])\n",
    "                    except ValueError:\n",
    "                        print('Its a value error')\n",
    "                        print(post['created_utc'])\n",
    "                        print(post_time)\n",
    "                    except Exception as ex:\n",
    "                        print('createdutc: ', post['created_utc'])\n",
    "                        print(ex)\n",
    "                        post_times = post_time.split()\n",
    "                        post_time = post_times[1]\n",
    "                        \n",
    "            hits = pd.DataFrame(hits)\n",
    "#             print('collected hits: ', hits.shape)\n",
    "            if not (hits.empty):\n",
    "                t = pd.to_datetime(hits.tweet_time, infer_datetime_format = True)\n",
    "                del hits['tweet_time']\n",
    "                hits['tweet_time'] = t\n",
    "                hits['td'] = (hits.post_time - hits.tweet_time).astype('timedelta64[h]')\n",
    "                hits = hits[(hits['td'] >= -672) & (hits['td'] <= 672)]\n",
    "#             print('After filtering: ', hits.shape)\n",
    "        combined_hits = combined_hits.append(hits, ignore_index = True)\n",
    "        count = count + 1\n",
    "        print(count)\n",
    "        print(combined_hits.shape)\n",
    "    return combined_hits\n",
    "\n",
    "def save_data(name, data):\n",
    "    data.to_csv('./results/group_by_author/'+name+'.csv')\n",
    "    \n",
    "def get_unique_elements_count(data, field):\n",
    "    counts = []\n",
    "    author_wise_lists = data[field].tolist()\n",
    "    for author_posts in author_wise_lists():\n",
    "        concatenated_list = []\n",
    "        for posts in author_posts:\n",
    "            concatenated_list = np.concatenate([concatenated_list, posts], axis=0)\n",
    "        unique_elements = np.unique(concatenated_list)\n",
    "        counts.append(len(unique_elements))\n",
    "    return counts\n",
    "\n",
    "def get_count(data, field):\n",
    "    counts = []\n",
    "    author_wise_lists = data[field].tolist()\n",
    "    for author_posts in author_wise_lists:\n",
    "        counts.append(len(author_posts))\n",
    "    return counts\n",
    "   \n",
    "authors_avg_time_difference = pd.DataFrame()\n",
    "ow_hits_per_author = pd.DataFrame()\n",
    "ow_posts_per_author = pd.DataFrame()\n",
    "ow_tweets_per_author = pd.DataFrame()\n",
    "\n",
    "tw_hits_per_author = pd.DataFrame()\n",
    "tw_posts_per_author = pd.DataFrame()\n",
    "tw_tweets_per_author = pd.DataFrame()\n",
    "\n",
    "om_hits_per_author = pd.DataFrame()\n",
    "om_posts_per_author = pd.DataFrame()\n",
    "om_tweets_per_author = pd.DataFrame()\n",
    "\n",
    "# different approach\n",
    "complete_hits = pd.DataFrame()\n",
    "for m in months:\n",
    "    relevant_hits = pd.read_csv('./results/hits/'+m+'.csv')\n",
    "    complete_hits = complete_hits.append(relevant_hits, ignore_index = True)\n",
    "    print('Number of hits in month ', m, 'with time difference less than a month :', (relevant_hits.shape)[0])\n",
    "\n",
    "owh = complete_hits[(complete_hits['td'] >= -168) & (complete_hits['td'] <= 168)]\n",
    "hits_per_author = owh.groupby(['post_author'])[\"post_id\"].count().reset_index(name=\"count\")\n",
    "ow_hits_per_author = ow_hits_per_author.append(hits_per_author, ignore_index = True)\n",
    "avg_td = owh.groupby(['post_author'])[\"td\"].mean().reset_index(name=\"average\")\n",
    "authors_avg_time_difference = authors_avg_time_difference.append(avg_td, ignore_index = True)\n",
    "posts_per_author = owh.groupby(['post_author'])[\"post_id\"].unique().reset_index(name=\"unique_post_ids\")\n",
    "ow_posts_per_author = ow_posts_per_author.append(posts_per_author, ignore_index = True)\n",
    "tweets_per_author = owh.groupby(['post_author'])[\"tweet_id\"].unique().reset_index(name=\"unique_tweet_ids\")\n",
    "ow_tweets_per_author = ow_tweets_per_author.append(tweets_per_author, ignore_index = True)\n",
    "\n",
    "twh = complete_hits[(complete_hits['td'] >= -336) & (complete_hits['td'] <= 336)]\n",
    "hits_per_author = twh.groupby(['post_author'])[\"post_id\"].count().reset_index(name=\"count\")\n",
    "tw_hits_per_author = tw_hits_per_author.append(hits_per_author, ignore_index = True)\n",
    "posts_per_author = twh.groupby(['post_author'])[\"post_id\"].unique().reset_index(name=\"unique_post_ids\")\n",
    "tw_posts_per_author = tw_posts_per_author.append(posts_per_author, ignore_index = True)\n",
    "tweets_per_author = twh.groupby(['post_author'])[\"tweet_id\"].unique().reset_index(name=\"unique_tweet_ids\")\n",
    "tw_tweets_per_author = tw_tweets_per_author.append(tweets_per_author, ignore_index = True)\n",
    "\n",
    "hits_per_author = complete_hits.groupby(['post_author'])[\"post_id\"].count().reset_index(name=\"count\")\n",
    "om_hits_per_author = om_hits_per_author.append(hits_per_author, ignore_index = True)\n",
    "posts_per_author = complete_hits.groupby(['post_author'])[\"post_id\"].unique().reset_index(name=\"unique_post_ids\")\n",
    "om_posts_per_author = om_posts_per_author.append(posts_per_author, ignore_index = True)\n",
    "tweets_per_author = complete_hits.groupby(['post_author'])[\"tweet_id\"].unique().reset_index(name=\"unique_tweet_ids\")\n",
    "om_tweets_per_author = om_tweets_per_author.append(tweets_per_author, ignore_index = True)\n",
    "\n",
    "plot_cdf([authors_avg_time_difference.average.tolist()], 'average time difference',leg=['within 1 week time difference'], path = './results/group_by_author/cdf_of_avgtd_per_author_for_one_week_td.pdf', islogx=False)\n",
    "\n",
    "plot_cdf([ow_hits_per_author['count'].tolist(), tw_hits_per_author['count'].tolist(), om_hits_per_author['count'].tolist()], 'hits/author',leg=['1 week time difference', '2 week time difference', '1 month time difference'], path = './results/group_by_author/cdf_of_hits_per_author_for_varying_td.pdf', islogx=True)\n",
    "\n",
    "ow_posts = get_count(ow_posts_per_author, 'unique_post_ids')\n",
    "tw_posts = get_count(tw_posts_per_author, 'unique_post_ids')\n",
    "om_posts = get_count(om_posts_per_author, 'unique_post_ids')\n",
    "\n",
    "plot_cdf([ow_posts, tw_posts, om_posts], 'posts/author',leg=['1 week time difference', '2 week time difference', '1 month time difference'], path = './results/group_by_author/cdf_of_posts_per_author_for_varying_td.pdf', islogx=True)\n",
    "\n",
    "ow_tweets = get_count(ow_tweets_per_author, 'unique_tweet_ids')\n",
    "tw_tweets = get_count(tw_tweets_per_author, 'unique_tweet_ids')\n",
    "om_tweets = get_count(om_tweets_per_author, 'unique_tweet_ids')\n",
    "\n",
    "plot_cdf([ow_tweets, tw_tweets, om_tweets], 'tweets/author',leg=['1 week time difference', '2 week time difference', '1 month time difference'], path = './results/group_by_author/cdf_of_tweets_per_author_for_varying_td.pdf', islogx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t_col = \"#235dba\"\n",
    "g_col = \"#005916\"\n",
    "c_col = \"#a50808\"\n",
    "r_col = \"#ff9900\"\n",
    "black = \"#000000\"\n",
    "pink = \"#f442f1\"\n",
    "t_ls = '-'\n",
    "r_ls = '--'\n",
    "c_ls = ':'\n",
    "g_ls = '-.'\n",
    "colors = [t_col, c_col, g_col, r_col, black, 'c', 'm', pink]\n",
    "line_styles = [t_ls, r_ls, c_ls, g_ls,t_ls, r_ls, c_ls, g_ls, t_ls]\n",
    "plot_cdf([authors_avg_time_difference.average.tolist()], 'average time difference',leg=['within 1 week time difference'], path = './results/group_by_author/cdf_of_avgtd_per_author_for_one_week_td.pdf', islogx=False)\n",
    "\n",
    "plot_cdf([ow_hits_per_author['count'].tolist(), tw_hits_per_author['count'].tolist(), om_hits_per_author['count'].tolist()], 'hits/author',leg=['1 week time difference', '2 week time difference', '1 month time difference'], path = './results/group_by_author/cdf_of_hits_per_author_for_varying_td.pdf', islogx=True)\n",
    "\n",
    "ow_posts = get_count(ow_posts_per_author, 'unique_post_ids')\n",
    "tw_posts = get_count(tw_posts_per_author, 'unique_post_ids')\n",
    "om_posts = get_count(om_posts_per_author, 'unique_post_ids')\n",
    "\n",
    "plot_cdf([ow_posts, tw_posts, om_posts], 'posts/author',leg=['1 week time difference', '2 week time difference', '1 month time difference'], path = './results/group_by_author/cdf_of_posts_per_author_for_varying_td.pdf', islogx=True)\n",
    "\n",
    "ow_tweets = get_count(ow_tweets_per_author, 'unique_tweet_ids')\n",
    "tw_tweets = get_count(tw_tweets_per_author, 'unique_tweet_ids')\n",
    "om_tweets = get_count(om_tweets_per_author, 'unique_tweet_ids')\n",
    "\n",
    "plot_cdf([ow_tweets, tw_tweets, om_tweets], 'tweets/author',leg=['1 week time difference', '2 week time difference', '1 month time difference'], path = './results/group_by_author/cdf_of_tweets_per_author_for_varying_td.pdf', islogx=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting, resolving and analysing URLs in tweet index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "index = pd.read_csv('./relevant_tweets.csv')\n",
    "index_tweet_ids = index['tweetid'].tolist()\n",
    "index_data = data[data['tweetid'].isin(index_tweet_ids)]\n",
    "index_data = index_data.drop_duplicates(subset='tweetid', keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(row):\n",
    "    links = re.findall(\"(?P<url>https?://[^\\s]+)\", str(row['tweet_text']))\n",
    "    return links\n",
    "\n",
    "def has_links(row):\n",
    "    links = re.findall(\"(?P<url>https?://[^\\s]+)\", str(row['tweet_text']))\n",
    "    if len(links)==0:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "index_data['has_urls'] = index_data.apply(has_links, axis=1)\n",
    "tweets_with_urls = index_data[index_data['has_urls'] == True]\n",
    "tweets_with_urls['urls'] = tweets_with_urls.apply(extract_urls, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_urls = []\n",
    "for l in tweets_with_urls['urls'].tolist():\n",
    "    all_urls = all_urls + l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "count = 0\n",
    "unresolved_urls = []\n",
    "resolved_urls = []\n",
    "for url in all_urls:\n",
    "    try:\n",
    "        resp = requests.head(url)\n",
    "        resolved_url = resp.headers[\"Location\"]\n",
    "        resolved_urls.append(resolved_url)\n",
    "        print(resolved_url)\n",
    "    except:\n",
    "        count = count + 1\n",
    "        print(count)\n",
    "        unresolved_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df = pd.DataFrame()\n",
    "url_df['url'] = resolved_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding information on the domain for each URL\n",
    "\n",
    "# Extracting the domain from the URL\n",
    "domain_re = re.compile(\"http[s]*://([^/]+)\")\n",
    "url_df[\"domain\"] = url_df.url.apply(lambda X: domain_re.search(X).group(1))\n",
    "\n",
    "# Removing leading www.\n",
    "www_re = re.compile(\"^www\\.\")\n",
    "url_df[\"domain\"] = url_df[\"domain\"].apply(lambda X: www_re.sub(\"\", X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_counts = url_df['domain'].value_counts()\n",
    "url_counts = url_df.groupby(['domain'])[\"url\"].count().reset_index(name=\"count\")\n",
    "url_counts = url_counts.sort_values('count', ascending=False)\n",
    "url_df.to_csv('./results/url_analysis/urls_domains.csv')\n",
    "url_counts.to_csv('./results/url_analysis/domain_counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get URL title and compare with tweet text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "        resp = requests.head(url)\n",
    "        resolved_url = resp.headers[\"Location\"]\n",
    "        resolved_urls.append(resolved_url)\n",
    "        print(resolved_url)\n",
    "    except:\n",
    "        count = count + 1\n",
    "        print(count)\n",
    "        unresolved_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_with_urls.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_schemes = ('http', 'ftp')\n",
    "allowed_exts = ('png', 'jpg')\n",
    "from urllib.parse import urlparse\n",
    "url = urlparse(\" https://en.insidesyriamc.com/2016/11/03/summary-of-armed-clashes-the-background-of-crisis\")\n",
    "print(url.scheme)\n",
    "url.scheme in allowed_schemes\n",
    "url.path.rsplit('.', 1)[1] in allowed_exts\n",
    "print(url.path.rsplit('/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text_url_titles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from urllib.parse import urlparse\n",
    "count = 0\n",
    "for index,row in tweets_with_urls.iterrows():\n",
    "    print(count)\n",
    "    count = count + 1\n",
    "    urls= row['urls']\n",
    "    for url in urls:\n",
    "        try:\n",
    "            resp = requests.head(url)\n",
    "            resolved_url = resp.headers[\"Location\"]\n",
    "            try:\n",
    "                if \"https://twitter.com\" not in resolved_url:\n",
    "                    u = urlparse(resolved_url)\n",
    "                    if u.netloc in [\"gopthedailydose.com\",\"en.insidesyriamc.com\"]:\n",
    "                        u_parts = u.path.rsplit('/')\n",
    "                        title = u_parts[4].replace(\"-\", \" \")\n",
    "                        tweet_text_url_titles.append({'tweetid': row['tweetid'],\n",
    "                                                         'tweet_text': row['tweet_text'],\n",
    "                                                         'url': resolved_url,\n",
    "                                                         'page_title': title})\n",
    "                        print('title from page: ', title)\n",
    "                        print('tweet text: ', row['tweet_text'])\n",
    "                    else:\n",
    "                        soup = BeautifulSoup(urllib.urlopen(resolved_url,timeout = 10))\n",
    "                        if soup.title != None:\n",
    "                            tweet_text_url_titles.append({'tweetid': row['tweetid'],\n",
    "                                                         'tweet_text': row['tweet_text'],\n",
    "                                                         'url': resolved_url,\n",
    "                                                         'page_title': soup.title.string})\n",
    "                            print('title from page: ', soup.title.string)\n",
    "                            print('tweet text: ', row['tweet_text'])\n",
    "            except Exception as e: \n",
    "                print('url when error: ',resolved_url)\n",
    "                print('tweet text: ', row['tweet_text'])\n",
    "                print('exception: ', e)\n",
    "        except Exception as e:\n",
    "            print('Error resolving: ', e)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_text = pd.DataFrame(tweet_text_url_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_text.to_csv('./results/url_new_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_removed_news = index_data[~index_data['tweetid'].isin(titles_text['tweetid'].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_removed_news.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat for more urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_with_urls2 = tweets_with_urls[~tweets_with_urls['tweetid'].isin(titles_text['tweetid'].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text_url_titles2 = []\n",
    "import string\n",
    "from urllib.parse import urlparse\n",
    "count = 0\n",
    "for index,row in tweets_with_urls2.iterrows():\n",
    "    print(count)\n",
    "    count = count + 1\n",
    "    urls= row['urls']\n",
    "    for url in urls:\n",
    "        try:\n",
    "            resp = requests.head(url)\n",
    "            resolved_url = resp.headers[\"Location\"]\n",
    "            tweet_text_url_titles2.append({'tweetid': row['tweetid'],\n",
    "                                                         'tweet_text': row['tweet_text'],\n",
    "                                                         'url': resolved_url})\n",
    "        except Exception as e:\n",
    "            print('Error resolving: ', e)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(tweet_text_url_titles2)\n",
    "d.to_csv('./results/for_annotation_url_titles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all the url part together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_urls = pd.read_csv('./results/annotated_url_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_urls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['is_matching'] = annotated_urls['is_matching'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = d[(d['is_matching'] == 1) | (d['is_matching'] == 11)]['tweetid'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_that_ar_not_page_titles = tweets_removed_news[~tweets_removed_news['tweetid'].isin(ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_that_ar_not_page_titles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_removed_news.to_csv('./results/index_tweets_after_removing_news_title_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = ids + titles_text['tweetid'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(set(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_df = pd.DataFrame()\n",
    "ids_df['tweetid'] = ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_df.to_csv('./results/tweet_ids_of_news_titles_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get popular authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['2016-01','2016-02','2016-03','2016-04','2016-05','2016-06',\n",
    "         '2016-07','2016-08','2016-09','2016-10','2016-11','2016-12',\n",
    "         '2017-01','2017-02','2017-03','2017-04','2017-05','2017-06',]\n",
    "complete_hits = pd.DataFrame()\n",
    "for m in months:\n",
    "    relevant_hits = pd.read_csv('./results/relevent_hits/'+m+'.csv')\n",
    "    complete_hits = complete_hits.append(relevant_hits, ignore_index = True)\n",
    "    print('Number of hits in month ', m, 'with time difference less than a month :', (relevant_hits.shape)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del complete_hits['Unnamed: 0']\n",
    "del complete_hits['bot_author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tweetids_df = pd.read_csv('./results/tweet_ids_of_news_titles_tweets.csv')\n",
    "title_tweetids = title_tweetids_df['tweetid'].tolist()\n",
    "print('complete hits before removing news titles: ', complete_hits.shape)\n",
    "complete_hits_without_news = complete_hits[~complete_hits['tweet_id'].isin(title_tweetids)]\n",
    "print('complete hits before after news titles: ', complete_hits_without_news.shape)\n",
    "# Remove authors with name as bot or auto\n",
    "def is_bot_auto(row):\n",
    "    author = str(row['post_author'])\n",
    "    if 'bot' in author:\n",
    "        return True\n",
    "    elif 'auto' in author:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "complete_hits_without_news['is_bot'] = complete_hits_without_news.apply(is_bot_auto, axis=1)\n",
    "complete_hits = complete_hits_without_news[~complete_hits_without_news['is_bot']==True]\n",
    "print('complete hits after removing bot authors: ', complete_hits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_per_author = complete_hits.groupby(['post_author'])[\"post_id\"].count().reset_index(name=\"count\")\n",
    "hits_per_author = hits_per_author.sort_values(by='count', ascending=False)\n",
    "hits_per_author = hits_per_author[hits_per_author['post_author'] != '[deleted]']\n",
    "popular_1000 = hits_per_author.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_per_author.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_status = []\n",
    "count = 0\n",
    "for author in popular_1000['post_author'].tolist():\n",
    "    \n",
    "    url = 'https://www.reddit.com/user/'+ author +'.json'\n",
    "    try:\n",
    "        resp = requests.get(url,headers = {'User-agent': 'your bot 0.1'})\n",
    "        author_status.append(resp.status_code)\n",
    "        count = count + 1\n",
    "        print('count', count)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(author)\n",
    "        author_status.append(None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_1000['author_status'] = author_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "suspended_authors = popular_1000[popular_1000['author_status']==403]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_404 = popular_1000[popular_1000['author_status']==404]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check if author is alive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "count = 0\n",
    "url = 'https://www.reddit.com/user/samacharbot2.json'\n",
    "try:\n",
    "    resp = requests.get(url,headers = {'User-agent': 'your bot 0.1'})\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    count = count + 1\n",
    "    print('count', count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popular subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = complete_hits.groupby(['subreddit'])[\"post_id\"].count().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = subreddits.sort_values(by='count', ascending=False)\n",
    "ps_100 = subreddits.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank authors by time difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_with_atleast_5_hits = pd.DataFrame()\n",
    "authors_with_atleast_5_hits = authors_with_atleast_5_hits.append(ow_hits_per_author[ow_hits_per_author['count'] >= 5], ignore_index = True)\n",
    "authors_with_atleast_5_hits = authors_with_atleast_5_hits.append(tw_hits_per_author[tw_hits_per_author['count'] >= 5], ignore_index = True)\n",
    "authors_with_atleast_5_hits = authors_with_atleast_5_hits.append(om_hits_per_author[om_hits_per_author['count'] >= 5], ignore_index = True)\n",
    "authors_with_atleast_5_hits = authors_with_atleast_5_hits.drop_duplicates(subset='post_author', keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "avg_tds = []\n",
    "mod_avg_tds = []\n",
    "for author in authors_with_atleast_5_hits['post_author'].tolist():\n",
    "    r_hits = complete_hits[complete_hits['post_author'] == author]\n",
    "    avg_tds.append(np.mean(r_hits['td'].tolist()))\n",
    "    mod_avg_tds.append(np.mean(np.abs(r_hits['td'].tolist())))\n",
    "    count = count + 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_with_atleast_5_hits['avg_td'] = avg_tds\n",
    "authors_with_atleast_5_hits['avg_abs_td'] = mod_avg_tds\n",
    "authors_with_atleast_5_hits = authors_with_atleast_5_hits.sort_values('avg_abs_td')\n",
    "authors_with_atleast_5_hits = authors_with_atleast_5_hit.rename(columns={'count': 'number_of_hits'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_with_atleast_5_hits.to_csv('./results/group_by_author/rank_by_td.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all hits of the top 5 authors and also their avg td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_authors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om_hits_per_author = om_hits_per_author.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "om_hits_per_author = popular_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in om_hits_per_author['post_author'].tolist()[0:6]:\n",
    "    if a != '[deleted]':\n",
    "        print(a)\n",
    "        top_5_authors.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_authors = list(set(top_5_authors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_top_5_authors = pd.DataFrame()\n",
    "for author in top_5_authors:\n",
    "    r_hits = complete_hits[complete_hits['post_author'] == author]\n",
    "    hits_top_5_authors = hits_top_5_authors.append(r_hits, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_top_5_authors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_td_top_5_author = hits_top_5_authors.groupby(['post_author'])[\"td\"].mean().reset_index(name=\"average_td\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_posts = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in months:\n",
    "    reddit_data = pd.read_csv('/INET/state-trolls/work/state-trolls/reddit_dataset/comments/posts/posts-' + m +'.csv')\n",
    "    fposts = reddit_data[reddit_data['id'].isin(hits_top_5_authors['post_id'].to_list())]\n",
    "    relevant_posts = relevant_posts.append(fposts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv('./relevant_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_posts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_posts(row):\n",
    "    post_id = row['post_id']\n",
    "    post = relevant_posts[relevant_posts['id'] == post_id]\n",
    "    return post['body'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subreddits(row):\n",
    "    post_id = row['post_id']\n",
    "    post = relevant_posts[relevant_posts['id'] == post_id]\n",
    "    return post['subreddit'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(row):\n",
    "    tweet_id = row['tweet_id']\n",
    "    tweet = twitter_data[twitter_data['tweetid'] == tweet_id]\n",
    "    return tweet['tweet_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_top_5_authors.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_top_5_authors['tweet_text'] = hits_top_5_authors.apply(get_tweets, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_top_5_authors = hits_top_5_authors.drop_duplicates(subset=['post_id','tweet_id'] , keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_top_5_authors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the percentage of suspension based on time difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['2016-01','2016-02','2016-03','2016-04','2016-05','2016-06',\n",
    "         '2016-07','2016-08','2016-09','2016-10','2016-11','2016-12',\n",
    "         '2017-01','2017-02','2017-03','2017-04','2017-05','2017-06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of hits in month  2016-01 with time difference less than a month : 16553\n",
      "Number of hits in month  2016-02 with time difference less than a month : 41170\n",
      "Number of hits in month  2016-03 with time difference less than a month : 51063\n",
      "Number of hits in month  2016-04 with time difference less than a month : 33508\n",
      "Number of hits in month  2016-05 with time difference less than a month : 37426\n",
      "Number of hits in month  2016-06 with time difference less than a month : 33341\n",
      "Number of hits in month  2016-07 with time difference less than a month : 42182\n",
      "Number of hits in month  2016-08 with time difference less than a month : 27679\n",
      "Number of hits in month  2016-09 with time difference less than a month : 33002\n",
      "Number of hits in month  2016-10 with time difference less than a month : 39516\n",
      "Number of hits in month  2016-11 with time difference less than a month : 48811\n",
      "Number of hits in month  2016-12 with time difference less than a month : 23263\n",
      "Number of hits in month  2017-01 with time difference less than a month : 34702\n",
      "Number of hits in month  2017-02 with time difference less than a month : 21026\n",
      "Number of hits in month  2017-03 with time difference less than a month : 26053\n",
      "Number of hits in month  2017-04 with time difference less than a month : 25606\n",
      "Number of hits in month  2017-05 with time difference less than a month : 55127\n",
      "Number of hits in month  2017-06 with time difference less than a month : 41310\n",
      "(631338, 9)\n",
      "(607528, 9)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "complete_hits = pd.DataFrame()\n",
    "for m in months:\n",
    "    relevant_hits = pd.read_csv('./results/relevent_hits/'+m+'.csv')\n",
    "    complete_hits = complete_hits.append(relevant_hits, ignore_index = True)\n",
    "    print('Number of hits in month ', m, 'with time difference less than a month :', (relevant_hits.shape)[0])\n",
    "del complete_hits['Unnamed: 0']\n",
    "del complete_hits['bot_author']\n",
    "print(complete_hits.shape)\n",
    "complete_hits = complete_hits.drop_duplicates(subset=['tweet_id','post_id'], keep='last')\n",
    "print(complete_hits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_hits_1y = pd.DataFrame()\n",
    "for m in months:\n",
    "    relevant_hits = pd.read_csv('./results/one_year/'+m+'.csv')\n",
    "    complete_hits_1y = complete_hits_1y.append(relevant_hits, ignore_index = True)\n",
    "    print('Number of hits in month ', m, 'with time difference less than a year :', (relevant_hits.shape)[0])\n",
    "del complete_hits_1y['Unnamed: 0']\n",
    "del complete_hits_1y['bot_author']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete hits before removing news titles:  (607528, 9)\n",
      "complete hits before after news titles:  (515508, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/INET/state-trolls/work/state-trolls/miniconda3/envs/env3/lib/python3.6/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete hits after removing bot authors:  (513666, 10)\n"
     ]
    }
   ],
   "source": [
    "title_tweetids_df = pd.read_csv('./results/tweet_ids_of_news_titles_tweets.csv')\n",
    "title_tweetids = title_tweetids_df['tweetid'].tolist()\n",
    "print('complete hits before removing news titles: ', complete_hits.shape)\n",
    "complete_hits_without_news = complete_hits[~complete_hits['tweet_id'].isin(title_tweetids)]\n",
    "print('complete hits before after news titles: ', complete_hits_without_news.shape)\n",
    "# Remove authors with name as bot or auto\n",
    "def is_bot_auto(row):\n",
    "    author = str(row['post_author'])\n",
    "    if 'bot' in author:\n",
    "        return True\n",
    "    elif 'auto' in author:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "complete_hits_without_news['is_bot'] = complete_hits_without_news.apply(is_bot_auto, axis=1)\n",
    "complete_hits = complete_hits_without_news[~complete_hits_without_news['is_bot']==True]\n",
    "print('complete hits after removing bot authors: ', complete_hits.shape)\n",
    "\n",
    "hits_per_author = complete_hits.groupby(['post_author'])[\"post_id\"].count().reset_index(name=\"count\")\n",
    "hits_per_author = hits_per_author.sort_values(by='count', ascending=False)\n",
    "hits_per_author = hits_per_author[hits_per_author['post_author'] != '[deleted]']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('complete hits before removing news titles: ', complete_hits_1y.shape)\n",
    "complete_hits_without_news_1y = complete_hits_1y[~complete_hits_1y['tweet_id'].isin(title_tweetids)]\n",
    "print('complete hits before after news titles: ', complete_hits_without_news_1y.shape)\n",
    "complete_hits_without_news_1y['is_bot'] = complete_hits_without_news_1y.apply(is_bot_auto, axis=1)\n",
    "complete_hits_1y = complete_hits_without_news_1y[~complete_hits_without_news_1y['is_bot']==True]\n",
    "print('complete hits after removing bot authors: ', complete_hits_1y.shape)\n",
    "\n",
    "hits_per_author_1y = complete_hits_1y.groupby(['post_author'])[\"post_id\"].count().reset_index(name=\"count\")\n",
    "hits_per_author_1y = hits_per_author_1y.sort_values(by='count', ascending=False)\n",
    "hits_per_author_1y = hits_per_author_1y[hits_per_author_1y['post_author'] != '[deleted]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/INET/state-trolls/work/state-trolls/miniconda3/envs/env3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "complete_hits['abs_td'] = complete_hits['td'].abs()\n",
    "avg_td = complete_hits.groupby(['post_author'])[\"abs_td\"].mean().reset_index(name=\"average\")\n",
    "avg_td_more_hits = avg_td[avg_td['post_author']\n",
    "                          .isin(hits_per_author[hits_per_author['count']>5]['post_author'].to_list())]\n",
    "avg_td_2w = avg_td_more_hits[(avg_td_more_hits['average'] >-1209600) & (avg_td_more_hits['average'] < 1209600)]\n",
    "avg_td_1w = avg_td_more_hits[(avg_td_more_hits['average'] >-604800) & (avg_td_more_hits['average'] < 604800)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many author with td 2w and hits >5 are suspended?\n",
    "import glob\n",
    "author_status = pd.DataFrame()\n",
    "for f in glob.glob('./results/author_status/*.csv'):\n",
    "    author_status = author_status.append(pd.read_csv(f),ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_status_1w_more_hits = author_status[author_status['post_author']\n",
    "                                           .isin(avg_td_1w['post_author'].tolist())]\n",
    "print(author_status_1w_more_hits[author_status_1w_more_hits['author_status'] == 403].shape)\n",
    "print(author_status_1w_more_hits[author_status_1w_more_hits['author_status'] == 404].shape)\n",
    "print(author_status_1w_more_hits[author_status_1w_more_hits['author_status'] == 200].shape)\n",
    "print(author_status_1w_more_hits[author_status_1w_more_hits['author_status'] == 503])\n",
    "print(author_status_1w_more_hits[author_status_1w_more_hits['author_status'] == 504])\n",
    "author_status_1w_more_hits['author_status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_status_1w_more_hits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_hits_1y['abs_td'] = complete_hits_1y['td'].abs()\n",
    "avg_td_1y = complete_hits_1y.groupby(['post_author'])[\"abs_td\"].mean().reset_index(name=\"average\")\n",
    "avg_td_1y_more_hits = avg_td_1y[avg_td_1y['post_author']\n",
    "                                .isin(hits_per_author_1y[hits_per_author_1y['count']>5]['post_author'].to_list())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many author with td 1 year and hits >5 are suspended?\n",
    "import glob\n",
    "author_status_1y = pd.DataFrame()\n",
    "for f in glob.glob('./results/one_year/author_status/*.csv'):\n",
    "    author_status_1y = author_status_1y.append(pd.read_csv(f),ignore_index=True)\n",
    "author_status_1y = author_status_1y.drop_duplicates(subset=['post_author'], keep='first')\n",
    "author_status_1y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_status_1y_more_hits = author_status_1y[author_status_1y['post_author']\n",
    "                                           .isin(avg_td_1y_more_hits['post_author'].tolist())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_status_1y_more_hits = author_status_1y_more_hits[~author_status_1y_more_hits['post_author'].isin(author_status_1w_more_hits['post_author'].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_status_1y_more_hits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(author_status_1y_more_hits[author_status_1y_more_hits['author_status'] == 403].shape)\n",
    "print(author_status_1y_more_hits[author_status_1y_more_hits['author_status'] == 404].shape)\n",
    "print(author_status_1y_more_hits[author_status_1y_more_hits['author_status'] == 200].shape)\n",
    "print(author_status_1y_more_hits[author_status_1y_more_hits['author_status'] == 503])\n",
    "print(author_status_1y_more_hits[author_status_1y_more_hits['author_status'] == 504])\n",
    "author_status_1y_more_hits['author_status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_samples = avg_td_1y.sample(n=30000)\n",
    "random_samples = random_samples[~random_samples['post_author'].isin(author_status_1w_more_hits['post_author'].tolist())]\n",
    "random_samples_author_status = author_status_1y[author_status_1y['post_author']\n",
    "                                           .isin(random_samples['post_author'].tolist())]\n",
    "print(random_samples_author_status.shape)\n",
    "random_samples_1000 = random_samples_author_status.sample(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_samples_1000[random_samples_1000['author_status'] == 403].shape)\n",
    "print(random_samples_1000[random_samples_1000['author_status'] == 404].shape)\n",
    "print(random_samples_1000[random_samples_1000['author_status'] == 200].shape)\n",
    "print(random_samples_1000[random_samples_1000['author_status'] == 503])\n",
    "print(random_samples_1000[random_samples_1000['author_status'] == 504])\n",
    "random_samples_1000['author_status'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all hits of top 5 authors ranked on td along with distinguished and sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/INET/state-trolls/work/state-trolls/miniconda3/envs/env3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "avg_td_1w['abs_td'] = avg_td_1w['average'].abs()\n",
    "avg_td_1w = avg_td_1w.sort_values('abs_td', ascending=True)\n",
    "top_1000_td = avg_td_1w#.head(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del avg_td_1w['abs_td']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = []\n",
    "for idx, row in top_1000_td.iterrows():\n",
    "    a = row['post_author']\n",
    "    h = author_status[author_status['post_author']==a]\n",
    "    status.append((h['author_status'].values)[0])\n",
    "#     print((h['count'].values)[0])\n",
    "top_1000_td['author_status'] = status  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = []\n",
    "for idx, row in top_1000_td.iterrows():\n",
    "    a = row['post_author']\n",
    "    h = hits_per_author[hits_per_author['post_author']==a]\n",
    "    counts.append((h['count'].values)[0])\n",
    "#     print((h['count'].values)[0])\n",
    "top_1000_td['counts'] = counts  \n",
    "top_1000_td = top_1000_td.sort_values('counts', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_author</th>\n",
       "      <th>average</th>\n",
       "      <th>author_status</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162434</th>\n",
       "      <td>m0rpheuz</td>\n",
       "      <td>16890.526316</td>\n",
       "      <td>404</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37571</th>\n",
       "      <td>GXGOW</td>\n",
       "      <td>17598.517241</td>\n",
       "      <td>200</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204185</th>\n",
       "      <td>yaqooberz</td>\n",
       "      <td>31707.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121860</th>\n",
       "      <td>bernbright-net</td>\n",
       "      <td>25739.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78138</th>\n",
       "      <td>PressXtoHuehue</td>\n",
       "      <td>29400.368421</td>\n",
       "      <td>404</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100064</th>\n",
       "      <td>TheRA51</td>\n",
       "      <td>18189.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116450</th>\n",
       "      <td>aknifeguy</td>\n",
       "      <td>11580.000000</td>\n",
       "      <td>404</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153444</th>\n",
       "      <td>jeevs193</td>\n",
       "      <td>12218.500000</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111356</th>\n",
       "      <td>Xenoanthropus</td>\n",
       "      <td>14263.125000</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101249</th>\n",
       "      <td>The_Silver_Avenger</td>\n",
       "      <td>26209.200000</td>\n",
       "      <td>200</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159335</th>\n",
       "      <td>lameeshalani</td>\n",
       "      <td>21099.133333</td>\n",
       "      <td>200</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31004</th>\n",
       "      <td>Edogawa4869</td>\n",
       "      <td>7450.266667</td>\n",
       "      <td>200</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94818</th>\n",
       "      <td>Suofficer</td>\n",
       "      <td>24734.538462</td>\n",
       "      <td>200</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13069</th>\n",
       "      <td>BitsyMonkey</td>\n",
       "      <td>21923.384615</td>\n",
       "      <td>200</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67862</th>\n",
       "      <td>N0puppet</td>\n",
       "      <td>16640.923077</td>\n",
       "      <td>200</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176501</th>\n",
       "      <td>porkfatonly</td>\n",
       "      <td>5325.750000</td>\n",
       "      <td>404</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201244</th>\n",
       "      <td>wellthisisnuts2015</td>\n",
       "      <td>8955.833333</td>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94341</th>\n",
       "      <td>StuckundFutz</td>\n",
       "      <td>21883.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99037</th>\n",
       "      <td>TheGenesect</td>\n",
       "      <td>27232.000000</td>\n",
       "      <td>404</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>11_25_13_TheEdge</td>\n",
       "      <td>19495.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153438</th>\n",
       "      <td>jeekaiy</td>\n",
       "      <td>6830.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48867</th>\n",
       "      <td>JForeIsBae</td>\n",
       "      <td>13557.833333</td>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30591</th>\n",
       "      <td>EWRNJ</td>\n",
       "      <td>16182.363636</td>\n",
       "      <td>404</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58446</th>\n",
       "      <td>LiveGoesOnline</td>\n",
       "      <td>20029.363636</td>\n",
       "      <td>403</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106349</th>\n",
       "      <td>UnoriginalGMan</td>\n",
       "      <td>17432.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64483</th>\n",
       "      <td>MillianaT</td>\n",
       "      <td>23254.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158896</th>\n",
       "      <td>kushmoney929</td>\n",
       "      <td>15922.800000</td>\n",
       "      <td>404</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123018</th>\n",
       "      <td>blackstonebite</td>\n",
       "      <td>27384.000000</td>\n",
       "      <td>404</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181669</th>\n",
       "      <td>rooozo</td>\n",
       "      <td>32001.300000</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114008</th>\n",
       "      <td>_Jett_</td>\n",
       "      <td>16094.800000</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158880</th>\n",
       "      <td>kurtca</td>\n",
       "      <td>34777.200000</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41735</th>\n",
       "      <td>Haktuar</td>\n",
       "      <td>9244.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113094</th>\n",
       "      <td>Zeke219</td>\n",
       "      <td>5661.900000</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28304</th>\n",
       "      <td>Donald__John_Trump</td>\n",
       "      <td>5685.000000</td>\n",
       "      <td>404</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73773</th>\n",
       "      <td>PC_Funpolice</td>\n",
       "      <td>27539.222222</td>\n",
       "      <td>200</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86145</th>\n",
       "      <td>Sard03</td>\n",
       "      <td>18904.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106761</th>\n",
       "      <td>V8Arwing93</td>\n",
       "      <td>12021.888889</td>\n",
       "      <td>200</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112690</th>\n",
       "      <td>ZZ34</td>\n",
       "      <td>25323.000000</td>\n",
       "      <td>403</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66866</th>\n",
       "      <td>Mr_Top_Hat_Jones-</td>\n",
       "      <td>22382.222222</td>\n",
       "      <td>200</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37415</th>\n",
       "      <td>GIGA_PUDDIN</td>\n",
       "      <td>29321.666667</td>\n",
       "      <td>200</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140225</th>\n",
       "      <td>fcrk</td>\n",
       "      <td>15368.888889</td>\n",
       "      <td>200</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169986</th>\n",
       "      <td>nicearthur32</td>\n",
       "      <td>7565.250000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>1iggy2</td>\n",
       "      <td>7458.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196864</th>\n",
       "      <td>trampabroad</td>\n",
       "      <td>20735.250000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55542</th>\n",
       "      <td>Kronicwar</td>\n",
       "      <td>31879.250000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77282</th>\n",
       "      <td>PointlessParable</td>\n",
       "      <td>11833.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53261</th>\n",
       "      <td>Kaprak</td>\n",
       "      <td>16316.250000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73978</th>\n",
       "      <td>PM_ME_CONCRETE</td>\n",
       "      <td>30382.500000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168593</th>\n",
       "      <td>mymindpsychee</td>\n",
       "      <td>25103.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200319</th>\n",
       "      <td>vourkosa</td>\n",
       "      <td>2797.250000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128873</th>\n",
       "      <td>clarrence-darrrow</td>\n",
       "      <td>16409.500000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155238</th>\n",
       "      <td>joules_</td>\n",
       "      <td>17448.500000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84325</th>\n",
       "      <td>Rubbydubbydoo</td>\n",
       "      <td>13650.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202573</th>\n",
       "      <td>wonderb0lt</td>\n",
       "      <td>15902.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63956</th>\n",
       "      <td>MiNdHaBiTs</td>\n",
       "      <td>33093.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107574</th>\n",
       "      <td>VibesAntagonist</td>\n",
       "      <td>12606.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37164</th>\n",
       "      <td>FutureMillionaire_</td>\n",
       "      <td>15590.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176888</th>\n",
       "      <td>prettyhelmet</td>\n",
       "      <td>14137.500000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60973</th>\n",
       "      <td>MadRDK</td>\n",
       "      <td>13357.500000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43326</th>\n",
       "      <td>HiImBanks</td>\n",
       "      <td>20745.285714</td>\n",
       "      <td>404</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75666</th>\n",
       "      <td>PelvisKick</td>\n",
       "      <td>29851.428571</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147105</th>\n",
       "      <td>hellotheremiss</td>\n",
       "      <td>23968.285714</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22067</th>\n",
       "      <td>CowboyBigBoss</td>\n",
       "      <td>5908.285714</td>\n",
       "      <td>404</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156963</th>\n",
       "      <td>kegaroo85</td>\n",
       "      <td>13538.571429</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103801</th>\n",
       "      <td>Trashy-TV-</td>\n",
       "      <td>11171.857143</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88132</th>\n",
       "      <td>ShadySingh</td>\n",
       "      <td>6380.571429</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181086</th>\n",
       "      <td>rkrish7</td>\n",
       "      <td>2436.714286</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191031</th>\n",
       "      <td>swimfast58</td>\n",
       "      <td>21087.571429</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120612</th>\n",
       "      <td>ballsackjohn</td>\n",
       "      <td>7702.142857</td>\n",
       "      <td>404</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175958</th>\n",
       "      <td>pm_me_pics_of_ur_nan</td>\n",
       "      <td>20621.142857</td>\n",
       "      <td>404</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90323</th>\n",
       "      <td>SlamOnTheMuzzleBrake</td>\n",
       "      <td>20253.857143</td>\n",
       "      <td>404</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97920</th>\n",
       "      <td>ThatOneHippyGuy</td>\n",
       "      <td>12942.142857</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52980</th>\n",
       "      <td>KageNoYugata</td>\n",
       "      <td>6586.142857</td>\n",
       "      <td>404</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126455</th>\n",
       "      <td>capgunfunk</td>\n",
       "      <td>33574.714286</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96031</th>\n",
       "      <td>TLDRify</td>\n",
       "      <td>18971.428571</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61306</th>\n",
       "      <td>MainStreetExile</td>\n",
       "      <td>14594.571429</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123443</th>\n",
       "      <td>blueberrysteven</td>\n",
       "      <td>11353.857143</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201207</th>\n",
       "      <td>welcomechallenge</td>\n",
       "      <td>12087.428571</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205593</th>\n",
       "      <td>zingbat</td>\n",
       "      <td>2463.333333</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145137</th>\n",
       "      <td>graphix62</td>\n",
       "      <td>32911.833333</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180270</th>\n",
       "      <td>rendrr</td>\n",
       "      <td>33892.333333</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77369</th>\n",
       "      <td>Polar_Ted</td>\n",
       "      <td>1991.333333</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133859</th>\n",
       "      <td>derrpy_derp</td>\n",
       "      <td>33734.666667</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93620</th>\n",
       "      <td>StepAnthonyWallyAli2</td>\n",
       "      <td>33466.666667</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7440</th>\n",
       "      <td>AnthonyRD</td>\n",
       "      <td>11580.000000</td>\n",
       "      <td>404</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147549</th>\n",
       "      <td>high_ride36</td>\n",
       "      <td>6490.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62513</th>\n",
       "      <td>Mathy16</td>\n",
       "      <td>29112.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98998</th>\n",
       "      <td>TheFuckingEagles</td>\n",
       "      <td>3807.333333</td>\n",
       "      <td>404</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158201</th>\n",
       "      <td>knittygnat</td>\n",
       "      <td>5875.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15129</th>\n",
       "      <td>Brewtus21</td>\n",
       "      <td>5939.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146465</th>\n",
       "      <td>happyantoninscalia</td>\n",
       "      <td>6414.000000</td>\n",
       "      <td>404</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113414</th>\n",
       "      <td>Zilosis</td>\n",
       "      <td>19358.666667</td>\n",
       "      <td>404</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78397</th>\n",
       "      <td>Probablynotclever</td>\n",
       "      <td>7223.333333</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173953</th>\n",
       "      <td>paralyyzed</td>\n",
       "      <td>9556.666667</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69867</th>\n",
       "      <td>Nilan666</td>\n",
       "      <td>16100.666667</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181268</th>\n",
       "      <td>robinpeterson90</td>\n",
       "      <td>1280.000000</td>\n",
       "      <td>403</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46874</th>\n",
       "      <td>ImNotGabriel</td>\n",
       "      <td>9790.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196913</th>\n",
       "      <td>trappymctrappish</td>\n",
       "      <td>15059.666667</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203953</th>\n",
       "      <td>xxBellum</td>\n",
       "      <td>10219.666667</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58337</th>\n",
       "      <td>LiterallyShaking</td>\n",
       "      <td>1082.666667</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 post_author       average  author_status  counts\n",
       "162434              m0rpheuz  16890.526316            404      38\n",
       "37571                  GXGOW  17598.517241            200      29\n",
       "204185             yaqooberz  31707.000000            200      24\n",
       "121860        bernbright-net  25739.000000            200      20\n",
       "78138         PressXtoHuehue  29400.368421            404      19\n",
       "100064               TheRA51  18189.000000            200      16\n",
       "116450             aknifeguy  11580.000000            404      16\n",
       "153444              jeevs193  12218.500000            200      16\n",
       "111356         Xenoanthropus  14263.125000            200      16\n",
       "101249    The_Silver_Avenger  26209.200000            200      15\n",
       "159335          lameeshalani  21099.133333            200      15\n",
       "31004            Edogawa4869   7450.266667            200      15\n",
       "94818              Suofficer  24734.538462            200      13\n",
       "13069            BitsyMonkey  21923.384615            200      13\n",
       "67862               N0puppet  16640.923077            200      13\n",
       "176501           porkfatonly   5325.750000            404      12\n",
       "201244    wellthisisnuts2015   8955.833333            200      12\n",
       "94341           StuckundFutz  21883.000000            200      12\n",
       "99037            TheGenesect  27232.000000            404      12\n",
       "936         11_25_13_TheEdge  19495.000000            200      12\n",
       "153438               jeekaiy   6830.000000            200      12\n",
       "48867             JForeIsBae  13557.833333            200      12\n",
       "30591                  EWRNJ  16182.363636            404      11\n",
       "58446         LiveGoesOnline  20029.363636            403      11\n",
       "106349        UnoriginalGMan  17432.000000            200      10\n",
       "64483              MillianaT  23254.000000            200      10\n",
       "158896          kushmoney929  15922.800000            404      10\n",
       "123018        blackstonebite  27384.000000            404      10\n",
       "181669                rooozo  32001.300000            200      10\n",
       "114008                _Jett_  16094.800000            200      10\n",
       "158880                kurtca  34777.200000            200      10\n",
       "41735                Haktuar   9244.000000            200      10\n",
       "113094               Zeke219   5661.900000            200      10\n",
       "28304     Donald__John_Trump   5685.000000            404      10\n",
       "73773           PC_Funpolice  27539.222222            200       9\n",
       "86145                 Sard03  18904.000000            200       9\n",
       "106761            V8Arwing93  12021.888889            200       9\n",
       "112690                  ZZ34  25323.000000            403       9\n",
       "66866      Mr_Top_Hat_Jones-  22382.222222            200       9\n",
       "37415            GIGA_PUDDIN  29321.666667            200       9\n",
       "140225                  fcrk  15368.888889            200       9\n",
       "169986          nicearthur32   7565.250000            200       8\n",
       "1367                  1iggy2   7458.000000            200       8\n",
       "196864           trampabroad  20735.250000            200       8\n",
       "55542              Kronicwar  31879.250000            200       8\n",
       "77282       PointlessParable  11833.000000            200       8\n",
       "53261                 Kaprak  16316.250000            200       8\n",
       "73978         PM_ME_CONCRETE  30382.500000            200       8\n",
       "168593         mymindpsychee  25103.000000            200       8\n",
       "200319              vourkosa   2797.250000            200       8\n",
       "128873     clarrence-darrrow  16409.500000            200       8\n",
       "155238               joules_  17448.500000            200       8\n",
       "84325          Rubbydubbydoo  13650.000000            200       8\n",
       "202573            wonderb0lt  15902.000000            200       8\n",
       "63956             MiNdHaBiTs  33093.000000            200       8\n",
       "107574       VibesAntagonist  12606.000000            200       8\n",
       "37164     FutureMillionaire_  15590.000000            200       8\n",
       "176888          prettyhelmet  14137.500000            200       8\n",
       "60973                 MadRDK  13357.500000            200       8\n",
       "43326              HiImBanks  20745.285714            404       7\n",
       "75666             PelvisKick  29851.428571            200       7\n",
       "147105        hellotheremiss  23968.285714            200       7\n",
       "22067          CowboyBigBoss   5908.285714            404       7\n",
       "156963             kegaroo85  13538.571429            200       7\n",
       "103801            Trashy-TV-  11171.857143            200       7\n",
       "88132             ShadySingh   6380.571429            200       7\n",
       "181086               rkrish7   2436.714286            200       7\n",
       "191031            swimfast58  21087.571429            200       7\n",
       "120612          ballsackjohn   7702.142857            404       7\n",
       "175958  pm_me_pics_of_ur_nan  20621.142857            404       7\n",
       "90323   SlamOnTheMuzzleBrake  20253.857143            404       7\n",
       "97920        ThatOneHippyGuy  12942.142857            200       7\n",
       "52980           KageNoYugata   6586.142857            404       7\n",
       "126455            capgunfunk  33574.714286            200       7\n",
       "96031                TLDRify  18971.428571            200       7\n",
       "61306        MainStreetExile  14594.571429            200       7\n",
       "123443       blueberrysteven  11353.857143            200       7\n",
       "201207      welcomechallenge  12087.428571            200       7\n",
       "205593               zingbat   2463.333333            200       6\n",
       "145137             graphix62  32911.833333            200       6\n",
       "180270                rendrr  33892.333333            200       6\n",
       "77369              Polar_Ted   1991.333333            200       6\n",
       "133859           derrpy_derp  33734.666667            200       6\n",
       "93620   StepAnthonyWallyAli2  33466.666667            200       6\n",
       "7440               AnthonyRD  11580.000000            404       6\n",
       "147549           high_ride36   6490.000000            200       6\n",
       "62513                Mathy16  29112.000000            200       6\n",
       "98998       TheFuckingEagles   3807.333333            404       6\n",
       "158201            knittygnat   5875.000000            200       6\n",
       "15129              Brewtus21   5939.000000            200       6\n",
       "146465    happyantoninscalia   6414.000000            404       6\n",
       "113414               Zilosis  19358.666667            404       6\n",
       "78397      Probablynotclever   7223.333333            200       6\n",
       "173953            paralyyzed   9556.666667            200       6\n",
       "69867               Nilan666  16100.666667            200       6\n",
       "181268       robinpeterson90   1280.000000            403       6\n",
       "46874           ImNotGabriel   9790.000000            200       6\n",
       "196913      trappymctrappish  15059.666667            200       6\n",
       "203953              xxBellum  10219.666667            200       6\n",
       "58337       LiterallyShaking   1082.666667            200       6"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_1000_td = top_1000_td.sort_values('average', ascending=True)\n",
    "t10 = top_1000_td.head(100)\n",
    "t10 = t10.sort_values('counts', ascending=False)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "t10.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(239, 2)\n"
     ]
    }
   ],
   "source": [
    "posts_per_author = complete_hits.groupby(['post_author'])[\"post_id\"].nunique().reset_index(name=\"count\")\n",
    "posts_per_author = posts_per_author[posts_per_author['post_author'] != '[deleted]']\n",
    "\n",
    "avg_td_more_posts = avg_td[avg_td['post_author']\n",
    "                          .isin(posts_per_author[posts_per_author['count']>5]['post_author'].to_list())]\n",
    "avg_td_posts_1w = avg_td_more_posts[(avg_td_more_posts['average'] >-604800) & (avg_td_more_posts['average'] < 604800)]\n",
    "print(avg_td_posts_1w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/INET/state-trolls/work/state-trolls/miniconda3/envs/env3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/INET/state-trolls/work/state-trolls/miniconda3/envs/env3/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "avg_td_posts_1w['abs_td'] = avg_td_posts_1w['average'].abs()\n",
    "top_1000_td_posts = avg_td_posts_1w#.head(1000)\n",
    "\n",
    "del avg_td_posts_1w['abs_td']\n",
    "status = []\n",
    "for idx, row in top_1000_td_posts.iterrows():\n",
    "    a = row['post_author']\n",
    "    h = author_status[author_status['post_author']==a]\n",
    "    status.append((h['author_status'].values)[0])\n",
    "#     print((h['count'].values)[0])\n",
    "top_1000_td_posts['author_status'] = status  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/INET/state-trolls/work/state-trolls/miniconda3/envs/env3/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "counts = []\n",
    "for idx, row in top_1000_td_posts.iterrows():\n",
    "    a = row['post_author']\n",
    "    h = posts_per_author[posts_per_author['post_author']==a]\n",
    "    counts.append((h['count'].values)[0])\n",
    "#     print((h['count'].values)[0])\n",
    "top_1000_td_posts['counts'] = counts  \n",
    "top_1000_td_posts = top_1000_td_posts.sort_values('counts', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_author</th>\n",
       "      <th>average</th>\n",
       "      <th>author_status</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172972</th>\n",
       "      <td>optimalg</td>\n",
       "      <td>60782.548387</td>\n",
       "      <td>200</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88272</th>\n",
       "      <td>Shanti_Ananda</td>\n",
       "      <td>202448.867925</td>\n",
       "      <td>200</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147135</th>\n",
       "      <td>helpmeredditimbored</td>\n",
       "      <td>338209.645161</td>\n",
       "      <td>200</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168998</th>\n",
       "      <td>nanami-773</td>\n",
       "      <td>327931.318584</td>\n",
       "      <td>200</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115969</th>\n",
       "      <td>afterpoop</td>\n",
       "      <td>337003.522727</td>\n",
       "      <td>200</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103932</th>\n",
       "      <td>Trenchguard</td>\n",
       "      <td>151414.766667</td>\n",
       "      <td>200</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192496</th>\n",
       "      <td>tgjer</td>\n",
       "      <td>216783.240000</td>\n",
       "      <td>200</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171010</th>\n",
       "      <td>nosecohn</td>\n",
       "      <td>430833.560000</td>\n",
       "      <td>200</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202017</th>\n",
       "      <td>wil_daven_</td>\n",
       "      <td>416340.170732</td>\n",
       "      <td>200</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84975</th>\n",
       "      <td>SFCHCP123</td>\n",
       "      <td>252481.714286</td>\n",
       "      <td>200</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48143</th>\n",
       "      <td>IsThisEvenEnglish</td>\n",
       "      <td>150760.312500</td>\n",
       "      <td>200</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13265</th>\n",
       "      <td>Blackhalo</td>\n",
       "      <td>440886.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35568</th>\n",
       "      <td>FlyntFlossy912</td>\n",
       "      <td>411514.312500</td>\n",
       "      <td>404</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83472</th>\n",
       "      <td>RoastedWithHoney</td>\n",
       "      <td>241452.800000</td>\n",
       "      <td>200</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13402</th>\n",
       "      <td>BlastechDevelopments</td>\n",
       "      <td>433562.461538</td>\n",
       "      <td>200</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180722</th>\n",
       "      <td>richierunner</td>\n",
       "      <td>72456.615385</td>\n",
       "      <td>200</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164043</th>\n",
       "      <td>matthank</td>\n",
       "      <td>164290.880000</td>\n",
       "      <td>403</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21716</th>\n",
       "      <td>Corax7</td>\n",
       "      <td>337762.900000</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48550</th>\n",
       "      <td>Ivanka_Humpalot</td>\n",
       "      <td>243480.800000</td>\n",
       "      <td>403</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>306MAGAnitudeQuake</td>\n",
       "      <td>380147.366667</td>\n",
       "      <td>404</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80694</th>\n",
       "      <td>Ramin_HAL9001</td>\n",
       "      <td>425768.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139584</th>\n",
       "      <td>eynul_sfinkter</td>\n",
       "      <td>418287.235294</td>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147782</th>\n",
       "      <td>hitbyacar1</td>\n",
       "      <td>311728.833333</td>\n",
       "      <td>200</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90603</th>\n",
       "      <td>SlothB77</td>\n",
       "      <td>430550.066667</td>\n",
       "      <td>200</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127491</th>\n",
       "      <td>chainsawx72</td>\n",
       "      <td>174720.951220</td>\n",
       "      <td>200</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201118</th>\n",
       "      <td>weedorb123</td>\n",
       "      <td>361668.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198750</th>\n",
       "      <td>uniqountry</td>\n",
       "      <td>455391.777778</td>\n",
       "      <td>200</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104721</th>\n",
       "      <td>Trumpster69</td>\n",
       "      <td>464318.428571</td>\n",
       "      <td>200</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98766</th>\n",
       "      <td>TheDon2016</td>\n",
       "      <td>383398.071429</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174542</th>\n",
       "      <td>pengo</td>\n",
       "      <td>365025.125000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192422</th>\n",
       "      <td>tethercat</td>\n",
       "      <td>357736.250000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203149</th>\n",
       "      <td>x888x</td>\n",
       "      <td>347571.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85991</th>\n",
       "      <td>SandorSNL</td>\n",
       "      <td>346086.194444</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142</th>\n",
       "      <td>99PercentTruth</td>\n",
       "      <td>438110.600000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38480</th>\n",
       "      <td>Geofferic</td>\n",
       "      <td>457530.857143</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116450</th>\n",
       "      <td>aknifeguy</td>\n",
       "      <td>11580.000000</td>\n",
       "      <td>404</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200052</th>\n",
       "      <td>virrul</td>\n",
       "      <td>334817.500000</td>\n",
       "      <td>404</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27939</th>\n",
       "      <td>DoctorWhosOnFirst</td>\n",
       "      <td>109747.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124375</th>\n",
       "      <td>boyo_america</td>\n",
       "      <td>226713.100000</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160277</th>\n",
       "      <td>letitraincandy</td>\n",
       "      <td>240335.615385</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957</th>\n",
       "      <td>Aevann</td>\n",
       "      <td>148241.588235</td>\n",
       "      <td>404</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190085</th>\n",
       "      <td>stufen1</td>\n",
       "      <td>269329.153846</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74084</th>\n",
       "      <td>PM_ME_TITS_N_KITTENS</td>\n",
       "      <td>121973.619048</td>\n",
       "      <td>200</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139604</th>\n",
       "      <td>ezinque</td>\n",
       "      <td>414066.476190</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81586</th>\n",
       "      <td>RecallRethuglicans</td>\n",
       "      <td>392467.769231</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149965</th>\n",
       "      <td>ifeelnothing69</td>\n",
       "      <td>292625.428571</td>\n",
       "      <td>404</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34932</th>\n",
       "      <td>Firefly54</td>\n",
       "      <td>235160.200000</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148355</th>\n",
       "      <td>hotsdiscordthrowaway</td>\n",
       "      <td>198399.571429</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148518</th>\n",
       "      <td>huadpe</td>\n",
       "      <td>432078.937500</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39904</th>\n",
       "      <td>Goodusernamebro</td>\n",
       "      <td>436657.142857</td>\n",
       "      <td>404</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6358</th>\n",
       "      <td>Amanoo</td>\n",
       "      <td>436988.142857</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25606</th>\n",
       "      <td>DeafDumbBlindBoy</td>\n",
       "      <td>339416.241379</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5511</th>\n",
       "      <td>Alasbabylon103</td>\n",
       "      <td>231162.285714</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87652</th>\n",
       "      <td>SerenasHairyBalls</td>\n",
       "      <td>459543.777778</td>\n",
       "      <td>403</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133190</th>\n",
       "      <td>deagesntwizzles</td>\n",
       "      <td>52560.285714</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97894</th>\n",
       "      <td>ThatIsNotMyMongoose</td>\n",
       "      <td>448006.375000</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148413</th>\n",
       "      <td>howdareyou</td>\n",
       "      <td>328469.476190</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12480</th>\n",
       "      <td>BigBossOfGondor</td>\n",
       "      <td>328229.888889</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57915</th>\n",
       "      <td>Lieutenant_Rans</td>\n",
       "      <td>281177.538462</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113654</th>\n",
       "      <td>Zorseking34</td>\n",
       "      <td>459066.520000</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43424</th>\n",
       "      <td>HighEnergyDaddy</td>\n",
       "      <td>91736.000000</td>\n",
       "      <td>404</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85430</th>\n",
       "      <td>SacredVoina</td>\n",
       "      <td>318624.142857</td>\n",
       "      <td>403</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127995</th>\n",
       "      <td>chicago_bunny</td>\n",
       "      <td>282072.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40390</th>\n",
       "      <td>GreatJanitor</td>\n",
       "      <td>447486.363636</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77567</th>\n",
       "      <td>PoopInMyBottom</td>\n",
       "      <td>425237.428571</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85967</th>\n",
       "      <td>SandersGirl2016</td>\n",
       "      <td>221511.333333</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179016</th>\n",
       "      <td>rasta_bomb</td>\n",
       "      <td>107437.000000</td>\n",
       "      <td>403</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136468</th>\n",
       "      <td>dryerlintcompelsyou</td>\n",
       "      <td>454344.333333</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144862</th>\n",
       "      <td>googajub</td>\n",
       "      <td>220339.571429</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46321</th>\n",
       "      <td>IcelandBestland</td>\n",
       "      <td>447977.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157934</th>\n",
       "      <td>kittycatrachel</td>\n",
       "      <td>175696.833333</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118528</th>\n",
       "      <td>appmanga</td>\n",
       "      <td>448800.333333</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36080</th>\n",
       "      <td>Fractal_Soul</td>\n",
       "      <td>413034.600000</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143946</th>\n",
       "      <td>ghostunit777</td>\n",
       "      <td>437507.785714</td>\n",
       "      <td>404</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20106</th>\n",
       "      <td>Ciridian</td>\n",
       "      <td>291643.833333</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18748</th>\n",
       "      <td>Chance4e</td>\n",
       "      <td>412967.000000</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72371</th>\n",
       "      <td>OldBenK3nobi</td>\n",
       "      <td>338057.333333</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167473</th>\n",
       "      <td>mportz</td>\n",
       "      <td>296669.500000</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200751</th>\n",
       "      <td>warm_kitchenette</td>\n",
       "      <td>303549.454545</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193916</th>\n",
       "      <td>thekarmagirl</td>\n",
       "      <td>303801.500000</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19480</th>\n",
       "      <td>Chilopodan</td>\n",
       "      <td>304203.571429</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122559</th>\n",
       "      <td>billycoolj</td>\n",
       "      <td>322715.250000</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51841</th>\n",
       "      <td>JorElofKrypton</td>\n",
       "      <td>325103.090909</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133074</th>\n",
       "      <td>ddiddy171</td>\n",
       "      <td>333590.714286</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155617</th>\n",
       "      <td>juche</td>\n",
       "      <td>273334.571429</td>\n",
       "      <td>403</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198669</th>\n",
       "      <td>underpopular</td>\n",
       "      <td>270150.647059</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81416</th>\n",
       "      <td>RealHumanHere</td>\n",
       "      <td>337865.352941</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49797</th>\n",
       "      <td>JanetYellensFuckboy</td>\n",
       "      <td>338140.800000</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43421</th>\n",
       "      <td>HighDagger</td>\n",
       "      <td>403946.166667</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161079</th>\n",
       "      <td>lla26</td>\n",
       "      <td>268190.500000</td>\n",
       "      <td>404</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64647</th>\n",
       "      <td>MinneapolisNick</td>\n",
       "      <td>263403.818182</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162749</th>\n",
       "      <td>madmaxq</td>\n",
       "      <td>256556.000000</td>\n",
       "      <td>404</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117884</th>\n",
       "      <td>angrybox1842</td>\n",
       "      <td>253600.090909</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>-Uranus--</td>\n",
       "      <td>362650.333333</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27749</th>\n",
       "      <td>DoMoreWithLess</td>\n",
       "      <td>363460.750000</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>912</th>\n",
       "      <td>111IIIlllIII</td>\n",
       "      <td>241204.750000</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>13angrymonkeys</td>\n",
       "      <td>388162.913043</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173353</th>\n",
       "      <td>over-my-head</td>\n",
       "      <td>391219.500000</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161981</th>\n",
       "      <td>lucaop</td>\n",
       "      <td>395047.250000</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123601</th>\n",
       "      <td>bm_57</td>\n",
       "      <td>390266.375000</td>\n",
       "      <td>404</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 post_author        average  author_status  counts\n",
       "172972              optimalg   60782.548387            200      61\n",
       "88272          Shanti_Ananda  202448.867925            200      51\n",
       "147135   helpmeredditimbored  338209.645161            200      42\n",
       "168998            nanami-773  327931.318584            200      40\n",
       "115969             afterpoop  337003.522727            200      33\n",
       "103932           Trenchguard  151414.766667            200      30\n",
       "192496                 tgjer  216783.240000            200      25\n",
       "171010              nosecohn  430833.560000            200      23\n",
       "202017            wil_daven_  416340.170732            200      23\n",
       "84975              SFCHCP123  252481.714286            200      21\n",
       "48143      IsThisEvenEnglish  150760.312500            200      16\n",
       "13265              Blackhalo  440886.000000            200      15\n",
       "35568         FlyntFlossy912  411514.312500            404      14\n",
       "83472       RoastedWithHoney  241452.800000            200      14\n",
       "13402   BlastechDevelopments  433562.461538            200      13\n",
       "180722          richierunner   72456.615385            200      12\n",
       "164043              matthank  164290.880000            403      11\n",
       "21716                 Corax7  337762.900000            200      10\n",
       "48550        Ivanka_Humpalot  243480.800000            403      10\n",
       "1902      306MAGAnitudeQuake  380147.366667            404      10\n",
       "80694          Ramin_HAL9001  425768.000000            200      10\n",
       "139584        eynul_sfinkter  418287.235294            200      10\n",
       "147782            hitbyacar1  311728.833333            200       9\n",
       "90603               SlothB77  430550.066667            200       9\n",
       "127491           chainsawx72  174720.951220            200       9\n",
       "201118            weedorb123  361668.000000            200       9\n",
       "198750            uniqountry  455391.777778            200       9\n",
       "104721           Trumpster69  464318.428571            200       9\n",
       "98766             TheDon2016  383398.071429            200       8\n",
       "174542                 pengo  365025.125000            200       8\n",
       "192422             tethercat  357736.250000            200       8\n",
       "203149                 x888x  347571.000000            200       8\n",
       "85991              SandorSNL  346086.194444            200       8\n",
       "3142          99PercentTruth  438110.600000            200       8\n",
       "38480              Geofferic  457530.857143            200       8\n",
       "116450             aknifeguy   11580.000000            404       8\n",
       "200052                virrul  334817.500000            404       8\n",
       "27939      DoctorWhosOnFirst  109747.000000            200       8\n",
       "124375          boyo_america  226713.100000            200       8\n",
       "160277        letitraincandy  240335.615385            200       8\n",
       "4957                  Aevann  148241.588235            404       8\n",
       "190085               stufen1  269329.153846            200       8\n",
       "74084   PM_ME_TITS_N_KITTENS  121973.619048            200       8\n",
       "139604               ezinque  414066.476190            200       7\n",
       "81586     RecallRethuglicans  392467.769231            200       7\n",
       "149965        ifeelnothing69  292625.428571            404       7\n",
       "34932              Firefly54  235160.200000            200       7\n",
       "148355  hotsdiscordthrowaway  198399.571429            200       7\n",
       "148518                huadpe  432078.937500            200       7\n",
       "39904        Goodusernamebro  436657.142857            404       7\n",
       "6358                  Amanoo  436988.142857            200       7\n",
       "25606       DeafDumbBlindBoy  339416.241379            200       7\n",
       "5511          Alasbabylon103  231162.285714            200       7\n",
       "87652      SerenasHairyBalls  459543.777778            403       7\n",
       "133190       deagesntwizzles   52560.285714            200       7\n",
       "97894    ThatIsNotMyMongoose  448006.375000            200       7\n",
       "148413            howdareyou  328469.476190            200       7\n",
       "12480        BigBossOfGondor  328229.888889            200       7\n",
       "57915        Lieutenant_Rans  281177.538462            200       7\n",
       "113654           Zorseking34  459066.520000            200       7\n",
       "43424        HighEnergyDaddy   91736.000000            404       7\n",
       "85430            SacredVoina  318624.142857            403       7\n",
       "127995         chicago_bunny  282072.000000            200       7\n",
       "40390           GreatJanitor  447486.363636            200       6\n",
       "77567         PoopInMyBottom  425237.428571            200       6\n",
       "85967        SandersGirl2016  221511.333333            200       6\n",
       "179016            rasta_bomb  107437.000000            403       6\n",
       "136468   dryerlintcompelsyou  454344.333333            200       6\n",
       "144862              googajub  220339.571429            200       6\n",
       "46321        IcelandBestland  447977.000000            200       6\n",
       "157934        kittycatrachel  175696.833333            200       6\n",
       "118528              appmanga  448800.333333            200       6\n",
       "36080           Fractal_Soul  413034.600000            200       6\n",
       "143946          ghostunit777  437507.785714            404       6\n",
       "20106               Ciridian  291643.833333            200       6\n",
       "18748               Chance4e  412967.000000            200       6\n",
       "72371           OldBenK3nobi  338057.333333            200       6\n",
       "167473                mportz  296669.500000            200       6\n",
       "200751      warm_kitchenette  303549.454545            200       6\n",
       "193916          thekarmagirl  303801.500000            200       6\n",
       "19480             Chilopodan  304203.571429            200       6\n",
       "122559            billycoolj  322715.250000            200       6\n",
       "51841         JorElofKrypton  325103.090909            200       6\n",
       "133074             ddiddy171  333590.714286            200       6\n",
       "155617                 juche  273334.571429            403       6\n",
       "198669          underpopular  270150.647059            200       6\n",
       "81416          RealHumanHere  337865.352941            200       6\n",
       "49797    JanetYellensFuckboy  338140.800000            200       6\n",
       "43421             HighDagger  403946.166667            200       6\n",
       "161079                 lla26  268190.500000            404       6\n",
       "64647        MinneapolisNick  263403.818182            200       6\n",
       "162749               madmaxq  256556.000000            404       6\n",
       "117884          angrybox1842  253600.090909            200       6\n",
       "377                -Uranus--  362650.333333            200       6\n",
       "27749         DoMoreWithLess  363460.750000            200       6\n",
       "912             111IIIlllIII  241204.750000            200       6\n",
       "1069          13angrymonkeys  388162.913043            200       6\n",
       "173353          over-my-head  391219.500000            200       6\n",
       "161981                lucaop  395047.250000            200       6\n",
       "123601                 bm_57  390266.375000            404       6"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_1000_td_posts = top_1000_td_posts.sort_values('average', ascending=True)\n",
    "t10 = top_1000_td_posts.head(100)\n",
    "t10 = t10.sort_values('counts', ascending=False)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "t10.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000 = top_1000_td['post_author'].head(1000).tolist()\n",
    "hits = complete_hits[complete_hits['post_author'].isin(top_1000)]\n",
    "post_ids = hits['post_id'].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_1000_td[top_1000_td['author_status']==403].shape)\n",
    "print(top_1000_td[top_1000_td['author_status']==404].shape)\n",
    "print(top_1000_td[top_1000_td['author_status']==200].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "y = [10, 3,4.2 , 4.57, 4.9,5]\n",
    "z = [20, 21,14.4 , 13.71, 14.4, 15.42]\n",
    "x = [1,2,3,4,5,6]\n",
    "\n",
    "# for xe, ye in zip(x, y):\n",
    "#     plt.scatter([xe] * len(ye), ye)\n",
    "\n",
    "plt.xticks([1, 2, 3, 4, 5 ,6])\n",
    "plt.axes().set_xticklabels(['top_10', 'top_100', 'top_500', 'top_700', 'top_1000', 'top_1400'])\n",
    "line = plt.plot(x, y, label = '% of suspension')\n",
    "# line.set_label('% of suspension')\n",
    "line2 = plt.plot(x, z, label = '% of deletion')\n",
    "# line2.set_label('% of deletion')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Top n authors\")\n",
    "plt.ylabel(\"% of suspension / deletion\")\n",
    "plt.savefig('./results/corelation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_1000_td.to_csv('./results/author_status/top_100_authors_status.csv')\n",
    "top_1000_td.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# related_posts = pd.DataFrame()\n",
    "for posts in glob.glob('../../reddit_dataset/comments/posts/posts-201*.csv'):\n",
    "    d = pd.read_csv(posts)\n",
    "    print(d.columns)\n",
    "    d = d[d['id'].isin(post_ids)]\n",
    "    related_posts = related_posts.append(d, ignore_index = True)\n",
    "    print('finished a file')\n",
    "print(related_posts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinguished = []\n",
    "for index,row in hits.iterrows():\n",
    "    post = related_posts[related_posts['id']==row['post_id']]\n",
    "    dis = (post['distinguished'].values)[0]\n",
    "#     print(dis)\n",
    "    distinguished.append(dis)\n",
    "hits['distinguished'] = distinguished    \n",
    "hits = hits[hits['distinguished']!='moderator']\n",
    "# hits.to_csv('./results/hits_od_top10_authors_ranked_by_td_hits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_10 = hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_100 = hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits_1000 = hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf([hits_10['abs_td'].tolist(),hits_100['abs_td'].tolist(), hits_1000['abs_td'].tolist() ],\n",
    "         'Time difference in seconds', './cdf_td.pdf', islogx=True, leg = ['top 10 authors', 'top 100 authors', 'top 1000 authors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_cdf(list_counts, xlabel, path, leg=False, islogx=True, xlimit=False):\n",
    "    t_col = \"#235dba\"\n",
    "    g_col = \"#005916\"\n",
    "    c_col = \"#a50808\"\n",
    "    r_col = \"#ff9900\"\n",
    "    black = \"#000000\"\n",
    "    pink = \"#f442f1\"\n",
    "    t_ls = '-'\n",
    "    r_ls = '--'\n",
    "    c_ls = ':'\n",
    "    g_ls = '-.'\n",
    "\n",
    "    markers = [\".\", \"o\", \"v\", \"^\", \"<\", \">\", \"1\", \"2\"]\n",
    "    colors = [t_col, c_col, g_col, r_col, black, 'c', 'm', pink]\n",
    "    line_styles = [t_ls, r_ls, c_ls, g_ls,t_ls, r_ls, c_ls, g_ls, t_ls]\n",
    "    colors = colors[1:]\n",
    "    line_styles= line_styles[1:]\n",
    "    while(len(list_counts) > len(colors)):\n",
    "        colors = colors + shuffle(colors)\n",
    "        line_styles = line_styles + shuffle(line_styles)\n",
    "        \n",
    "    if xlimit:\n",
    "        l2 = []\n",
    "        for l in list_counts:\n",
    "            l2_1 = [x for x in l if x<=xlimit]\n",
    "            l2.append(l2_1)\n",
    "        list_counts = l2\n",
    "    \n",
    "    for l in list_counts:\n",
    "        l.sort()\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    yvals = []\n",
    "    for l in list_counts:\n",
    "        yvals.append(np.arange(len(l))/float(len(l)-1))\n",
    "    for i in range(len(list_counts)):\n",
    "        ax.plot(list_counts[i], yvals[i], color=colors[i], linestyle=line_styles[i])\n",
    "    if islogx:\n",
    "        ax.set_xscale(\"log\")\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('CDF')\n",
    "    plt.grid()\n",
    "    for item in ([ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(13)\n",
    "    \n",
    "    if leg:\n",
    "        plt.legend(leg, loc='best', fontsize=13)\n",
    "    \n",
    "    plt.show()\n",
    "    fig.savefig(path, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = hits.groupby(['subreddit'])[\"post_id\"].count().reset_index(name=\"count\")\n",
    "subreddits = subreddits.sort_values('count', ascending=False)\n",
    "subreddits.to_csv('./results/top_subreddits_for_top10_authors_rankeed_td_hits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_hits_2w = complete_hits[(complete_hits['td'] >-1209600) & (complete_hits['td'] < 1209600)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits_2w = complete_hits_2w.groupby(['subreddit'])[\"post_id\"].count().reset_index(name=\"count\")\n",
    "subreddits_2w = subreddits_2w.sort_values('count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline of 1000 samples within 5y td for percentage of suspension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one = pd.read_csv('results/five_year/2016-01.csv')\n",
    "two = pd.read_csv('./results/five_year/2016-02.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one=one.append(two, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tweetids_df = pd.read_csv('./results/tweet_ids_of_news_titles_tweets.csv')\n",
    "title_tweetids = title_tweetids_df['tweetid'].tolist()\n",
    "print('complete hits before removing news titles: ', one.shape)\n",
    "one_without_news = one[~one['tweet_id'].isin(title_tweetids)]\n",
    "print('complete hits before after news titles: ', one_without_news.shape)\n",
    "# Remove authors with name as bot or auto\n",
    "def is_bot_auto(row):\n",
    "    author = str(row['post_author'])\n",
    "    if 'bot' in author:\n",
    "        return True\n",
    "    elif 'auto' in author:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "one_without_news['is_bot'] = one_without_news.apply(is_bot_auto, axis=1)\n",
    "one = one_without_news[~one_without_news['is_bot']==True]\n",
    "print('complete hits after removing bot authors: ', one.shape)\n",
    "\n",
    "hits_per_author_one = one.groupby(['post_author'])[\"post_id\"].count().reset_index(name=\"count\")\n",
    "hits_per_author_one = hits_per_author_one.sort_values(by='count', ascending=False)\n",
    "hits_per_author_one = hits_per_author_one[hits_per_author_one['post_author'] != '[deleted]']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = one.sample(n=10000)\n",
    "samples = samples[~samples['post_author'].isin(author_status_1w_more_hits['post_author'].tolist())]\n",
    "# random_samples_author_status = author_status_1y[author_status_1y['post_author']\n",
    "#                                            .isin(random_samples['post_author'].tolist())]\n",
    "# print(random_samples_author_status.shape)\n",
    "# random_samples_1000 = random_samples_author_status.sample(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples.sample(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "author_status = []\n",
    "s = 0\n",
    "for author in samples['post_author'].tolist():\n",
    "        print(s)\n",
    "        s = s + 1\n",
    "        url = 'https://www.reddit.com/user/'+ author +'.json'\n",
    "        try:\n",
    "            resp = requests.get(url,headers = {'User-agent': 'your bot 0.1'})\n",
    "            author_status.append(resp.status_code)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(author)\n",
    "            author_status.append(None)\n",
    "samples['author_status'] = author_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples[samples['author_status'] == 403].shape)\n",
    "print(samples[samples['author_status'] == 404].shape)\n",
    "print(samples[samples['author_status'] == 200].shape)\n",
    "print(samples[samples['author_status'] == 503])\n",
    "print(samples[samples['author_status'] == 504])\n",
    "samples['author_status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_hits[complete_hits['post_author'] == '_Steve64comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting reddit users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('banned_reddit_users.txt', 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.split('\\n')\n",
    "newdata = []\n",
    "for l in data:\n",
    "    l = l.split('|')\n",
    "    u = l[1].split('/')\n",
    "    if len(u)>1:\n",
    "        newdata.append(u[1])\n",
    "        print(u[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banned_users = pd.DataFrame()\n",
    "banned_users['author'] = newdata\n",
    "banned_users.to_csv('./results/banned_suspecious_users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if users are present in hits that we obtained\n",
    "complete_hits = pd.DataFrame()\n",
    "for m in months:\n",
    "    relevant_hits = pd.read_csv('./results/relevent_hits/'+m+'.csv')\n",
    "    complete_hits = complete_hits.append(relevant_hits, ignore_index = True)\n",
    "    print('Number of hits in month ', m, 'with time difference less than a month :', (relevant_hits.shape)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_hits[relevant_hits['post_author'].isin(newdata)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.DataFrame()\n",
    "for m in months:\n",
    "    relevant_posts = pd.read_csv('/INET/state-trolls/work/state-trolls/reddit_dataset/comments/posts/posts-'+m+'.csv')\n",
    "    matching_authors = relevant_posts[relevant_posts['author'].isin(newdata)]\n",
    "    print(m, matching_authors.shape)\n",
    "    authors = authors.append(matching_authors, ignore_index=True)\n",
    "#     print(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_hits = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def process_authors(row):\n",
    "    auth = row['post_author']\n",
    "#     if isinstance(auth,str):\n",
    "#         auth = ast.literal_eval(auth)\n",
    "    auth = auth.split()\n",
    "#     print(auth[1])\n",
    "    return auth[1]\n",
    "iteration=0\n",
    "for f in glob.glob('../../reddit_dataset/comments/scores/RC_2017-05.bz2.decompressed/*.txt'):\n",
    "    iteration = iteration + 1\n",
    "    with open(f , 'r') as content_file:\n",
    "        content = content_file.read()\n",
    "        json_data = content.replace('][',',')\n",
    "        j_object = json.loads(json_data)\n",
    "        json_df = pd.DataFrame(j_object)\n",
    "#         json_df['author'] = json_df.apply(process_authors,axis=1)\n",
    "        authors_hits = authors_hits.append(json_df[json_df['post_id'].isin(authors['id'].tolist())])\n",
    "        print(authors_hits.shape)\n",
    "        print(iteration)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ids = authors_hits['post_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_posts = pd.DataFrame()\n",
    "for m in months:\n",
    "    relevant_posts = pd.read_csv('/INET/state-trolls/work/state-trolls/reddit_dataset/comments/posts/posts-'+m+'.csv')\n",
    "    posts = relevant_posts[relevant_posts['id'].isin(p_ids)]\n",
    "\n",
    "    related_posts = related_posts.append(posts, ignore_index=True)\n",
    "    print(posts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_hits.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('./relevant_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author(row):\n",
    "    post = related_posts[related_posts['id'] == row['post_id']]\n",
    "    row['post_author'] = post['author'].tolist()[0]\n",
    "    row['post_body'] = post['body'].tolist()[0]\n",
    "    return row\n",
    "    print(row)\n",
    "k = authors_hits.apply(get_author,axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in authors['author'].unique():\n",
    "    s = k[k['post_author']==a]\n",
    "    print(s['cosine_similarity'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= []\n",
    "def get_tweet(row):\n",
    "    t = tweets[tweets['tweetid']==row['tweet_id']]\n",
    "    return t['tweet_text'].tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k['tweet_text'] = k.apply(get_tweet, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.to_csv('./results/hits_suspecious_users.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get relavent hits by relaxing the threshold but of selected themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_data = pd.read_csv('./relevant_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_tweetids_df = pd.read_csv('./results/tweet_ids_of_news_titles_tweets.csv')\n",
    "title_tweetids = title_tweetids_df['tweetid'].tolist()\n",
    "print('complete hits before removing news titles: ', complete_hits.shape)\n",
    "complete_hits_without_news = complete_hits[~complete_hits['tweet_id'].isin(title_tweetids)]\n",
    "print('complete hits before after news titles: ', complete_hits_without_news.shape)\n",
    "# Remove authors with name as bot or auto\n",
    "def is_bot_auto(row):\n",
    "    author = str(row['post_author'])\n",
    "    if 'bot' in author:\n",
    "        return True\n",
    "    elif 'auto' in author:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "complete_hits_without_news['is_bot'] = complete_hits_without_news.apply(is_bot_auto, axis=1)\n",
    "complete_hits = complete_hits_without_news[~complete_hits_without_news['is_bot']==True]\n",
    "print('complete hits after removing bot authors: ', complete_hits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# author status baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aas = author_status_1w_more_hits['post_author'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.DataFrame()\n",
    "for chunk in pd.read_json('../../reddit_dataset/comments/RC_2016-01.bz2.decompressed',lines = True, chunksize=10000):\n",
    "    chunks = chunk\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks['author'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_hits = pd.DataFrame()\n",
    "for f in glob.glob('./results/baseline_hits/*.csv'):\n",
    "    df = pd.read_csv(f)\n",
    "    df = df.drop_duplicates(subset=['post_author'], keep='first')\n",
    "    random_hits = random_hits.append(df, ignore_index=True)\n",
    "    print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chunks.shape)\n",
    "random_hits = chunks.drop_duplicates(subset=['author'], keep='first')\n",
    "random_hits = chunks[~chunks['author'].isin(aas)]\n",
    "print(chunks.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_set = chunks.sample(1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_author(row):\n",
    "#     a = row['post_author']\n",
    "#     auth = a.split()\n",
    "#     print(auth[1])\n",
    "#     return auth[1]\n",
    "# random_set['author'] = random_set.apply(get_author, axis=1)\n",
    "baseline_authors = random_set['author'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statuses = []\n",
    "count = 0\n",
    "for author in baseline_authors:\n",
    "    url = 'https://www.reddit.com/user/'+ author +'.json'\n",
    "    try:\n",
    "        resp = requests.get(url,headers = {'User-agent': 'your bot 0.1'})\n",
    "        statuses.append(resp.status_code)\n",
    "        count = count + 1\n",
    "        print('count', count)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(author)\n",
    "        statuses.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_set['author_status'] = statuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_set[random_set['author_status']==403].shape)\n",
    "print(random_set[random_set['author_status']==404].shape)\n",
    "print(random_set[random_set['author_status']==200].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_set['author_status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
