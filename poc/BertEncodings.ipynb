{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# displays all columns and rows when asked to print\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading all the datasets and extracting only english tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/INET/state-trolls/work/state-trolls/env1/lib/python3.5/site-packages/ipykernel_launcher.py:12: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if sys.path[0] == '':\n",
      " 25%|██▌       | 1/4 [00:01<00:04,  1.52s/it]/INET/state-trolls/work/state-trolls/env1/lib/python3.5/site-packages/ipykernel_launcher.py:12: DtypeWarning: Columns (15,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if sys.path[0] == '':\n",
      " 50%|█████     | 2/4 [00:07<00:05,  2.75s/it]/INET/state-trolls/work/state-trolls/env1/lib/python3.5/site-packages/ipykernel_launcher.py:12: DtypeWarning: Columns (30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if sys.path[0] == '':\n",
      "100%|██████████| 4/4 [00:55<00:00, 13.77s/it]\n",
      "/INET/state-trolls/work/state-trolls/env1/lib/python3.5/site-packages/ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in the dataset:  9995700\n",
      "Number of English tweets in the dataset:  3739633\n"
     ]
    }
   ],
   "source": [
    "# list of all the dataset files\n",
    "dataset_paths = [\"../datasets/russia_052020_tweets_csv_hashed_2.csv\", \n",
    "         \"../datasets/russian_linked_tweets_csv_hashed.csv\", \n",
    "         \"../datasets/ira_tweets_csv_hashed.csv\", \n",
    "         \"../datasets/russia_201906_1_tweets_csv_hashed.csv\"]\n",
    "\n",
    "# path to store the entire combined dataset\n",
    "combined_dataset_path = \"../datasets/russian_trolls.csv\"\n",
    "\n",
    "# returns a pandas dataframe consisting of entries from all the dataset files\n",
    "def get_combined_dataset(paths):\n",
    "    data = pd.concat((pd.read_csv(file) for file in tqdm(paths)))\n",
    "    return data\n",
    "\n",
    "data = get_combined_dataset(dataset_paths)\n",
    "print(\"Number of tweets in the dataset: \", data.shape[0])\n",
    "\n",
    "# extracts just the english tweets by using the language tag\n",
    "is_english_tweet = data['tweet_language'] == 'en'\n",
    "english_data = data[is_english_tweet]\n",
    "\n",
    "print(\"Number of English tweets in the dataset: \", english_data.shape[0])\n",
    "english_tweet_data = english_data[['tweetid', 'tweet_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the tweets to remove mentions, urls and retweet string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of english tweets after preprocessing:  3736616\n"
     ]
    }
   ],
   "source": [
    "def remove_url(tweet):\n",
    "    result = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    return result\n",
    "\n",
    "def remove_mentions(tweet):\n",
    "    result = re.sub(r\"@\\S+\", \"\", tweet)\n",
    "    return result\n",
    "\n",
    "def remove_retweet(tweet):\n",
    "    result = re.sub(r\"RT @\\S+\", \"\", tweet)\n",
    "    return result\n",
    "\n",
    "# takes list of tweets as input and returns list of pre-processed tweets as output\n",
    "def preprocess(tweets):\n",
    "    processed_tweets = []\n",
    "    for tweet in tweets:\n",
    "        result = remove_mentions(remove_retweet(remove_url(tweet)))\n",
    "        processed_tweets.append(result)\n",
    "    return processed_tweets\n",
    "\n",
    "tweets = english_tweet_data['tweet_text']\n",
    "tweets = preprocess(tweets)\n",
    "\n",
    "english_tweet_data = english_tweet_data.assign(processed_tweets = tweets)\n",
    "\n",
    "# removes the entries having empty string after preprocessing\n",
    "is_not_empty_string = english_tweet_data['processed_tweets'].apply(lambda x: not str.isspace(x))\n",
    "english_tweet_data = english_tweet_data[is_not_empty_string]\n",
    "\n",
    "english_tweet_data = english_tweet_data.reset_index()\n",
    "\n",
    "print(\"Number of english tweets after preprocessing: \", english_tweet_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the embeddings for all the tweets using Bert As Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the tweet encodings from the tweet texts.\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodings = bc.encode(english_tweet_data['processed_tweets'].to_list())\n",
    "print(\"Number of dimensions in the encodings: \",encodings.shape[1])\n",
    "\n",
    "# save the encodings for later use. Order preserved\n",
    "np.save('tweet_encodings_flag_true', encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the encodings of tweets previously save\n",
    "loaded_encodings_flag_true = np.load('tweet_encodings_flag_true.npy')\n",
    "loaded_encodings_flag_false = np.load('tweet_encodings_flag_false.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Faiss to index search to obtain top k similar tweets for given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "dimension = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for embeddings obtained using mask_cls flag set to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the index:  3736616\n"
     ]
    }
   ],
   "source": [
    "index_true = faiss.IndexFlatL2(dimension)\n",
    "index_true.add(loaded_encodings_flag_true)                  # add encodings to the index\n",
    "print(\"Number of entries in the index: \", index_true.ntotal)\n",
    "\n",
    "# random tweet encoded to query on the index of entire dataset\n",
    "query = bc.encode(english_tweet_data.tail(1)['processed_tweets'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10                          # number of nearest neighbours to be fetched\n",
    "D, I = index_true.search(query, k)     # actual search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query:  You got us! We really did help  \n",
      "\n",
      "The top 10 results obtained were: \n",
      "\n",
      "You got us! We really did help \n",
      " U r welcome! we r spreading out info about police brutality! help us and make the world better!\n",
      "we all gonna be scared to death!! I hate govt for it! It was planned before!!! #phosphorusdisaster\n",
      "Do we really need this? I bet we can do better shows, we already have them! #AtlantaFX #Empire \n",
      "Unguarded we are now! we all gonna be scared to death!! #phosphorusdisaster\n",
      " America is facing GOOD vs EVIL! This election so critical! We pray GOD help us &amp; help America! #MAGA 🚂🇺🇸  \n",
      " No amnesty!  We, the American people come first ! Not fair for those who did it right!! #AmericaFirst Democrats are willin…\n",
      "It`s #Iran who need our help! They must agree on anything we want to give them! #KerryDoSmth\n",
      "I know, that cops are well trained to shoot! It was planned beforehand!! #CopsWillBeCops\n",
      " GET OUT &amp; VOTE for TRUMP like your Country Depends on Him! We Do!! Let's take OUR COUNTRY Back! #MAGA 🇺🇸🇺🇸 #GodBlessUSA 💝 h…\n"
     ]
    }
   ],
   "source": [
    "print(\"The query: \", english_tweet_data['processed_tweets'].iloc[3736615], \"\\n\")\n",
    "print(\"The top 10 results obtained were: \\n\")\n",
    "for i in I[0]:\n",
    "    print(english_tweet_data['processed_tweets'].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for embeddings obtained using mask_cls flag set to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the index:  3736616\n"
     ]
    }
   ],
   "source": [
    "index_false = faiss.IndexFlatL2(dimension)\n",
    "index_false.add(loaded_encodings_flag_false)                  # add encodings to the index\n",
    "print(\"Number of entries in the index: \", index_false.ntotal)\n",
    "\n",
    "\n",
    "# random tweet encoded to query on the index of entire dataset\n",
    "query = np.asarray([loaded_encodings_flag_false[3736615]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10                          # number of nearest neighbours to be fetched\n",
    "D, I = index_false.search(query, k)     # actual search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query:  You got us! We really did help  \n",
      "\n",
      "The top 10 results obtained were: \n",
      "\n",
      "You got us! We really did help \n",
      " you really got it!\n",
      " you really got it!\n",
      " We did it, guys 🌮🖖🏼🦄🌪🌯🍾\n",
      " You survived! \n",
      "   We did it! \n",
      " We got you   \n",
      " We got you   \n",
      " We did it, ladies \n",
      " APPLAUD YOU! YOU GUYS MADE THIS POSSIBLE! \n"
     ]
    }
   ],
   "source": [
    "print(\"The query: \", english_tweet_data['processed_tweets'].iloc[3736615], \"\\n\")\n",
    "print(\"The top 10 results obtained were: \\n\")\n",
    "for i in I[0]:\n",
    "    print(english_tweet_data['processed_tweets'].iloc[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
