{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# displays all columns and rows when asked to print\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading all the datasets and extracting only english tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]/INET/state-trolls/work/state-trolls/env1/lib/python3.5/site-packages/ipykernel_launcher.py:12: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if sys.path[0] == '':\n",
      " 25%|██▌       | 1/4 [00:01<00:04,  1.54s/it]/INET/state-trolls/work/state-trolls/env1/lib/python3.5/site-packages/ipykernel_launcher.py:12: DtypeWarning: Columns (15,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if sys.path[0] == '':\n",
      " 50%|█████     | 2/4 [00:06<00:05,  2.53s/it]/INET/state-trolls/work/state-trolls/env1/lib/python3.5/site-packages/ipykernel_launcher.py:12: DtypeWarning: Columns (30) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if sys.path[0] == '':\n",
      "100%|██████████| 4/4 [00:54<00:00, 13.50s/it]\n",
      "/INET/state-trolls/work/state-trolls/env1/lib/python3.5/site-packages/ipykernel_launcher.py:12: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets in the dataset:  9995700\n",
      "Number of English tweets in the dataset:  3739633\n"
     ]
    }
   ],
   "source": [
    "# list of all the dataset files\n",
    "dataset_paths = [\"../datasets/russia_052020_tweets_csv_hashed_2.csv\", \n",
    "         \"../datasets/russian_linked_tweets_csv_hashed.csv\", \n",
    "         \"../datasets/ira_tweets_csv_hashed.csv\", \n",
    "         \"../datasets/russia_201906_1_tweets_csv_hashed.csv\"]\n",
    "\n",
    "# path to store the entire combined dataset\n",
    "combined_dataset_path = \"../datasets/russian_trolls.csv\"\n",
    "\n",
    "# returns a pandas dataframe consisting of entries from all the dataset files\n",
    "def get_combined_dataset(paths):\n",
    "    data = pd.concat((pd.read_csv(file) for file in tqdm(paths)))\n",
    "    return data\n",
    "\n",
    "data = get_combined_dataset(dataset_paths)\n",
    "print(\"Number of tweets in the dataset: \", data.shape[0])\n",
    "\n",
    "# extracts just the english tweets by using the language tag\n",
    "is_english_tweet = data['tweet_language'] == 'en'\n",
    "english_data = data[is_english_tweet]\n",
    "\n",
    "print(\"Number of English tweets in the dataset: \", english_data.shape[0])\n",
    "english_tweet_data = english_data[['tweetid', 'tweet_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the tweets to remove mentions, urls and retweet string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of english tweets after preprocessing:  3736616\n"
     ]
    }
   ],
   "source": [
    "def remove_url(tweet):\n",
    "    result = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    return result\n",
    "\n",
    "def remove_mentions(tweet):\n",
    "    result = re.sub(r\"@\\S+\", \"\", tweet)\n",
    "    return result\n",
    "\n",
    "def remove_retweet(tweet):\n",
    "    result = re.sub(r\"RT @\\S+\", \"\", tweet)\n",
    "    return result\n",
    "\n",
    "# takes list of tweets as input and returns list of pre-processed tweets as output\n",
    "def preprocess(tweets):\n",
    "    processed_tweets = []\n",
    "    for tweet in tweets:\n",
    "        result = remove_mentions(remove_retweet(remove_url(tweet)))\n",
    "        processed_tweets.append(result)\n",
    "    return processed_tweets\n",
    "\n",
    "tweets = english_tweet_data['tweet_text']\n",
    "tweets = preprocess(tweets)\n",
    "\n",
    "english_tweet_data = english_tweet_data.assign(processed_tweets = tweets)\n",
    "\n",
    "# removes the entries having empty string after preprocessing\n",
    "is_not_empty_string = english_tweet_data['processed_tweets'].apply(lambda x: not str.isspace(x))\n",
    "english_tweet_data = english_tweet_data[is_not_empty_string]\n",
    "\n",
    "english_tweet_data = english_tweet_data.reset_index()\n",
    "\n",
    "print(\"Number of english tweets after preprocessing: \", english_tweet_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the embeddings for all the tweets using Bert As Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the tweet encodings from the tweet texts.\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodings = bc.encode(english_tweet_data['processed_tweets'].to_list())\n",
    "# print(\"Number of dimensions in the encodings: \",encodings.shape[1])\n",
    "\n",
    "# # save the encodings for later use. Order preserved\n",
    "# np.save('tweet_encodings_flag_true', encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the encodings of tweets previously save\n",
    "loaded_encodings_flag_true = np.load('tweet_encodings_flag_true.npy')\n",
    "loaded_encodings_flag_false = np.load('tweet_encodings_flag_false.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Faiss to index search to obtain top k similar tweets for given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "dimension = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for embeddings obtained using mask_cls flag set to True and using IndexFlatL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the index:  3736616\n"
     ]
    }
   ],
   "source": [
    "index_true = faiss.IndexFlatL2(dimension)\n",
    "index_true.add(loaded_encodings_flag_true)                  # add encodings to the index\n",
    "print(\"Number of entries in the index: \", index_true.ntotal)\n",
    "\n",
    "# random tweet encoded to query on the index of entire dataset\n",
    "query_true = bc.encode(english_tweet_data.tail(1)['processed_tweets'].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20                          # number of nearest neighbours to be fetched\n",
    "D, I = index_true.search(query_true, k)     # actual search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query:  You got us! We really did help  \n",
      "\n",
      "The top 10 results obtained were: \n",
      "\n",
      "You got us! We really did help \n",
      " U r welcome! we r spreading out info about police brutality! help us and make the world better!\n",
      "we all gonna be scared to death!! I hate govt for it! It was planned before!!! #phosphorusdisaster\n",
      "Do we really need this? I bet we can do better shows, we already have them! #AtlantaFX #Empire \n",
      "Unguarded we are now! we all gonna be scared to death!! #phosphorusdisaster\n",
      " America is facing GOOD vs EVIL! This election so critical! We pray GOD help us &amp; help America! #MAGA 🚂🇺🇸  \n",
      " No amnesty!  We, the American people come first ! Not fair for those who did it right!! #AmericaFirst Democrats are willin…\n",
      "It`s #Iran who need our help! They must agree on anything we want to give them! #KerryDoSmth\n",
      "I know, that cops are well trained to shoot! It was planned beforehand!! #CopsWillBeCops\n",
      " GET OUT &amp; VOTE for TRUMP like your Country Depends on Him! We Do!! Let's take OUR COUNTRY Back! #MAGA 🇺🇸🇺🇸 #GodBlessUSA 💝 h…\n",
      " GET OUT &amp; VOTE for TRUMP like your Country Depends on Him! We Do!! Let's take OUR COUNTRY Back! #MAGA 🇺🇸🇺🇸 #GodBlessUSA 💝 h…\n",
      " Hey GOP Congress, get this done!! This is why we worked for you and elected you! \n",
      "\n",
      "\n",
      " Hey GOP Congress, get this done!! This is why we worked for you and elected you!   \n",
      "I hate govt for it! It was planned before!!! What will our government do to protect us? #phosphorusdisaster\n",
      " I just torched 1500  cals in fitness class! Now time 2 eat!    \n",
      " Judge Jeanine: We can enforce immigration laws &amp; it’s time we did!     #USA🇺🇸\n",
      " Where are we marching??! #TrumpHotel #DC #ShutItDown We, You, ALL of us are the ones who can stop their fascist regime!…\n",
      "   we have the momentum back now! Let's get #lyingted out after NY! \n",
      " WOW! My mom &amp; I were talking about the riots last week! My ex husband remembers tanks rolling down our street! \n",
      " I Love These Guys, They Work 24/7 Protecting Us &amp; R Way Of Life!! These Are The Real Heroes!!\n",
      "\n",
      "#tcot #ccot #pjnet \n"
     ]
    }
   ],
   "source": [
    "print(\"The query: \", english_tweet_data['processed_tweets'].iloc[3736615], \"\\n\")\n",
    "print(\"The top 10 results obtained were: \\n\")\n",
    "for i in I[0]:\n",
    "    print(english_tweet_data['processed_tweets'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for execution:  2.278086434180538  minutes\n"
     ]
    }
   ],
   "source": [
    "# top 100 for 1000 queries\n",
    "def get_nearest_neighbour_matrix(queries,index, k):\n",
    "    D, I = index.search(queries, k)\n",
    "    similarity_matrix = pd.DataFrame(I)\n",
    "    score_matrix = pd.DataFrame(D)\n",
    "    query_tweets = english_tweet_data['processed_tweets'].iloc[0:len(queries)].tolist()\n",
    "    nearest_neighbour_matrix = pd.DataFrame().assign(queries = query_tweets)\n",
    "    rank = 1\n",
    "    for (col_name, col_data) in similarity_matrix.iteritems():\n",
    "        indexes = col_data.values\n",
    "        similar_tweets = english_tweet_data['processed_tweets'].iloc[indexes].tolist()\n",
    "        nearest_neighbour_matrix['top_'+str(rank)] = similar_tweets\n",
    "        nearest_neighbour_matrix['scores_'+str(rank)] = score_matrix[rank-1]\n",
    "        rank = rank + 1\n",
    "    return nearest_neighbour_matrix\n",
    "start_time = time.perf_counter()\n",
    "queries = loaded_encodings_flag_false[0:1000]\n",
    "start_time = time.perf_counter()\n",
    "nearest_neighbour_matrix = get_nearest_neighbour_matrix(queries, index_true, 100)\n",
    "end_time = time.perf_counter()\n",
    "print(\"Time taken for execution: \", (end_time - start_time)/60.0 ,\" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbour_matrix.to_csv('./nearest_neighbour_matrix_flag_true.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for embeddings obtained using mask_cls flag set to True and using IndexFlatIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the index:  3736616\n",
      "Time taken for execution:  2.1954765672485035  minutes\n"
     ]
    }
   ],
   "source": [
    "queries = loaded_encodings_flag_false[0:1000]\n",
    "index_true_flatIP = faiss.IndexFlatIP(dimension)\n",
    "index_true_flatIP.add(loaded_encodings_flag_true)                  # add encodings to the index\n",
    "print(\"Number of entries in the index: \", index_true_flatIP.ntotal)\n",
    "start_time = time.perf_counter()\n",
    "nearest_neighbour_matrix = get_nearest_neighbour_matrix(queries, index_true_flatIP, 100)\n",
    "end_time = time.perf_counter()\n",
    "print(\"Time taken for execution: \", (end_time - start_time)/60.0 ,\" minutes\")\n",
    "nearest_neighbour_matrix.to_csv('./nearest_neighbour_matrix_flag_true_IndexFlatIP.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for embeddings obtained using mask_cls flag set to False and using IndexFlatL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the index:  3736616\n"
     ]
    }
   ],
   "source": [
    "index_false = faiss.IndexFlatL2(dimension)\n",
    "index_false.add(loaded_encodings_flag_false)                  # add encodings to the index\n",
    "print(\"Number of entries in the index: \", index_false.ntotal)\n",
    "\n",
    "\n",
    "# random tweet encoded to query on the index of entire dataset\n",
    "query = np.asarray([loaded_encodings_flag_false[3736615]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20                          # number of nearest neighbours to be fetched\n",
    "D, I = index_false.search(query, k)     # actual search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query:  You got us! We really did help  \n",
      "\n",
      "The top 10 results obtained were: \n",
      "\n",
      "You got us! We really did help \n",
      " you really got it!\n",
      " you really got it!\n",
      " We did it, guys 🌮🖖🏼🦄🌪🌯🍾\n",
      " You survived! \n",
      "   We did it! \n",
      " We got you   \n",
      " We got you   \n",
      " We did it, ladies \n",
      " APPLAUD YOU! YOU GUYS MADE THIS POSSIBLE! \n",
      "   We did it dude!\n",
      "   We did it dude!\n",
      " We did!\n",
      "We survived thanks obama        \n",
      " We made it y'all!\n",
      "  I appreciated it!\n",
      " I got you! \n",
      " I got you! \n",
      " You nailed it!\n",
      "   YOU nailed it! \n"
     ]
    }
   ],
   "source": [
    "print(\"The query: \", english_tweet_data['processed_tweets'].iloc[3736615], \"\\n\")\n",
    "print(\"The top 10 results obtained were: \\n\")\n",
    "for i in I[0]:\n",
    "    print(english_tweet_data['processed_tweets'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for execution:  2.311331648255388  minutes\n"
     ]
    }
   ],
   "source": [
    "# top 100 for 1000 queries\n",
    "start_time = time.perf_counter()\n",
    "queries = loaded_encodings_flag_false[0:1000]\n",
    "nearest_neighbour_matrix = get_nearest_neighbour_matrix(queries, index_false, 100)\n",
    "end_time = time.perf_counter()\n",
    "print(\"Time taken for execution: \", (end_time - start_time)/60.0 ,\" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbour_matrix.to_csv('./nearest_neighbour_matrix_flag_false.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for embeddings obtained using mask_cls flag set to False and using IndexFlatIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the index:  3736616\n",
      "Time taken for execution:  2.377830154076219  minutes\n"
     ]
    }
   ],
   "source": [
    "queries = loaded_encodings_flag_false[0:1000]\n",
    "index_false_flatIP = faiss.IndexFlatIP(dimension)\n",
    "index_false_flatIP.add(loaded_encodings_flag_false)                  # add encodings to the index\n",
    "print(\"Number of entries in the index: \", index_false_flatIP.ntotal)\n",
    "start_time = time.perf_counter()\n",
    "nearest_neighbour_matrix = get_nearest_neighbour_matrix(queries, index_false_flatIP, 100)\n",
    "end_time = time.perf_counter()\n",
    "print(\"Time taken for execution: \", (end_time - start_time)/60.0 ,\" minutes\")\n",
    "nearest_neighbour_matrix.to_csv('./nearest_neighbour_matrix_flag_false_IndexFlatIP.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Cosine similarity using Bert Encodings and comparing its results with that of Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes cposine similarity between two vectors\n",
    "def cosine_similarity(a,b):\n",
    "    dot = np.dot(a, b)\n",
    "    norma = np.linalg.norm(a)\n",
    "    normb = np.linalg.norm(b)\n",
    "    cos = dot / (norma * normb)\n",
    "    return cos\n",
    "\n",
    "# returns the list of tweets for given list of indices\n",
    "def get_list_of_tweets(indexes):\n",
    "    tweets = []\n",
    "    for i in indexes[0]:\n",
    "        tweets.append(english_tweet_data['processed_tweets'].iloc[i])\n",
    "    return tweets\n",
    "\n",
    "# normalizes the vectors in ndarray row wise\n",
    "def normalize_rows(x: np.ndarray):\n",
    "    return x/np.linalg.norm(x, ord=2, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_index = 223456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''constructs a dataframe to compare the scores and results\n",
    "obtained from Faiss and Cosine similarity score computed on own'''\n",
    "\n",
    "index_false_flatIP = faiss.IndexFlatIP(dimension)\n",
    "index_false_flatIP.add(loaded_encodings_flag_false)     \n",
    "query = np.asarray([loaded_encodings_flag_false[query_index]])\n",
    "k = 20                          # number of nearest neighbours to be fetched\n",
    "D, I = index_false_flatIP.search(query, k)     # actual search \n",
    "j = 0\n",
    "faiss_scores = []\n",
    "for i in I[0]:\n",
    "    faiss_scores.append(D[0][j])\n",
    "    j = j + 1\n",
    "top_20_tweets = get_list_of_tweets(I)\n",
    "score_comparison = pd.DataFrame().assign(top_20_tweets_from_faiss_without_vector_normalization = top_20_tweets)\n",
    "score_comparison['Faiss cosine similarity scores'] = faiss_scores\n",
    "\n",
    "query = np.asarray([loaded_encodings_flag_false[query_index]])\n",
    "dist_out = 1-pairwise_distances(query, loaded_encodings_flag_false, metric=\"cosine\")\n",
    "cosine_scores = pd.DataFrame().assign(scores=dist_out[0])\n",
    "cosine_scores.sort_values(by = ['scores'],inplace = True, ascending = False)\n",
    "score_comparison['top 20 tweets from computing cosine similarity on own'] = get_list_of_tweets([cosine_scores.head(20).index])\n",
    "score_comparison['cosine similarity scores'] = cosine_scores['scores'][0:20].tolist()\n",
    "\n",
    "normalized_encodings = normalize_rows(loaded_encodings_flag_false)\n",
    "index_false_flatIP = faiss.IndexFlatIP(dimension)\n",
    "index_false_flatIP.add(normalized_encodings)     \n",
    "query = np.asarray([normalized_encodings[query_index]])\n",
    "k = 20                          # number of nearest neighbours to be fetched\n",
    "D, I = index_false_flatIP.search(query, k)     # actual search \n",
    "j = 0\n",
    "faiss_scores = []\n",
    "for i in I[0]:\n",
    "    faiss_scores.append(D[0][j])\n",
    "    j = j + 1\n",
    "top_20_tweets = get_list_of_tweets(I)\n",
    "score_comparison['top 20 tweets from Faiss with normalized vector as index'] = top_20_tweets\n",
    "score_comparison['scores'] = faiss_scores\n",
    "score_comparison.to_csv('./result_comparison_between_mask_cls_false_faissIndexIP_and_own_cosine_similarities_3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''constructs a dataframe to compare the scores and results\n",
    "obtained from Faiss and Cosine similarity score computed on own'''\n",
    "\n",
    "index_true_flatIP = faiss.IndexFlatIP(dimension)\n",
    "index_true_flatIP.add(loaded_encodings_flag_true)     \n",
    "query = np.asarray([loaded_encodings_flag_true[query_index]])\n",
    "k = 20                          # number of nearest neighbours to be fetched\n",
    "D, I = index_true_flatIP.search(query, k)     # actual search \n",
    "j = 0\n",
    "faiss_scores = []\n",
    "for i in I[0]:\n",
    "    faiss_scores.append(D[0][j])\n",
    "    j = j + 1\n",
    "top_20_tweets = get_list_of_tweets(I)\n",
    "score_comparison = pd.DataFrame().assign(top_20_tweets_from_faiss_without_vector_normalization = top_20_tweets)\n",
    "score_comparison['Faiss cosine similarity scores'] = faiss_scores\n",
    "\n",
    "query = np.asarray([loaded_encodings_flag_true[query_index]])\n",
    "dist_out = 1-pairwise_distances(query, loaded_encodings_flag_true, metric=\"cosine\")\n",
    "cosine_scores = pd.DataFrame().assign(scores=dist_out[0])\n",
    "cosine_scores.sort_values(by = ['scores'],inplace = True, ascending = False)\n",
    "score_comparison['top 20 tweets from computing cosine similarity on own'] = get_list_of_tweets([cosine_scores.head(20).index])\n",
    "score_comparison['cosine similarity scores'] = cosine_scores['scores'][0:20].tolist()\n",
    "\n",
    "normalized_encodings = normalize_rows(loaded_encodings_flag_true)\n",
    "index_true_flatIP = faiss.IndexFlatIP(dimension)\n",
    "index_true_flatIP.add(normalized_encodings)     \n",
    "query = np.asarray([normalized_encodings[query_index]])\n",
    "k = 20                          # number of nearest neighbours to be fetched\n",
    "D, I = index_true_flatIP.search(query, k)     # actual search \n",
    "j = 0\n",
    "faiss_scores = []\n",
    "for i in I[0]:\n",
    "    faiss_scores.append(D[0][j])\n",
    "    j = j + 1\n",
    "top_20_tweets = get_list_of_tweets(I)\n",
    "score_comparison['top 20 tweets from Faiss with normalized vector as index'] = top_20_tweets\n",
    "score_comparison['scores'] = faiss_scores\n",
    "score_comparison.to_csv('./result_comparison_between_mask_cls_true_faissIndexIP_and_own_cosine_similarities_3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
